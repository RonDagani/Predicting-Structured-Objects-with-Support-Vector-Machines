{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGhbbJFUe85s"
      },
      "outputs": [],
      "source": [
        "def loss_augment_unaries(unary_potentials, y,class_weight):\n",
        "    n_states = unary_potentials.shape[1]\n",
        "    for i in range(unary_potentials.shape[0]):\n",
        "        for s in range(n_states):\n",
        "            if s == y[i]:\n",
        "                continue\n",
        "            unary_potentials[i, s] += class_weight[y[i]]\n",
        "def lp_general_graph(unaries, edges, edge_weights):\n",
        "    if unaries.shape[1] != edge_weights.shape[1]:\n",
        "        raise ValueError(\"incompatible shapes of unaries\"\n",
        "                         \" and edge_weights.\")\n",
        "    if edge_weights.shape[1] != edge_weights.shape[2]:\n",
        "        raise ValueError(\"Edge weights not square!\")\n",
        "    if edge_weights.shape[0] != edges.shape[0]:\n",
        "        raise ValueError(\"Number of edge weights different from number of\"\n",
        "                         \"edges\")\n",
        "\n",
        "    n_nodes, n_states = map(int, unaries.shape)\n",
        "    n_edges = len(edges)\n",
        "\n",
        "    # variables: n_nodes * n_states for nodes,\n",
        "    # n_edges * n_states ** 2 for edges\n",
        "    n_variables = n_nodes * n_states + n_edges * n_states ** 2\n",
        "\n",
        "    # constraints: one per node,\n",
        "    # and n_nodes * n_states for pairwise minus one redundant per edge\n",
        "    n_constraints = n_nodes + n_edges * (2 * n_states - 1)\n",
        "\n",
        "    # offset to get to the edge variables in columns\n",
        "    edges_offset = n_nodes * n_states\n",
        "    # store constraints as triple (data, I, J)\n",
        "    data, I, J = [], [], []\n",
        "\n",
        "    # summation constraints\n",
        "    for i in range(n_nodes):\n",
        "        for j in range(n_states):\n",
        "            data.append(1)\n",
        "            I.append(i)\n",
        "            J.append(i * n_states + j)\n",
        "            #constraints[i, i * n_states + j] = 1\n",
        "    # we row_idx tracks constraints = rows in constraint matrix\n",
        "    row_idx = n_nodes\n",
        "    # edge marginalization constraint\n",
        "    for i in range(2 * n_edges * n_states):\n",
        "        edge = i // (2 * n_states)\n",
        "        state = (i % n_states)\n",
        "        vertex_in_edge = i % (2 * n_states) // n_states\n",
        "        vertex = edges[edge][vertex_in_edge]\n",
        "        if vertex_in_edge == 1 and state == n_states - 1:\n",
        "            # the last summation constraint is redundant.\n",
        "            continue\n",
        "        # for one vertex iterate over all states of the other vertex\n",
        "        #[row_idx, int(vertex) * n_states + state] = -1\n",
        "        data.append(-1)\n",
        "        I.append(row_idx)\n",
        "        J.append(int(vertex) * n_states + state)\n",
        "        edge_var_index = edges_offset + edge * n_states ** 2\n",
        "        if vertex_in_edge == 0:\n",
        "            # first vertex in edge\n",
        "            for j in range(n_states):\n",
        "                data.append(1)\n",
        "                I.append(row_idx)\n",
        "                J.append(edge_var_index + state * n_states + j)\n",
        "                #[row_idx, edge_var_index + state * n_states + j] = 1\n",
        "        else:\n",
        "            # second vertex in edge\n",
        "            for j in range(n_states):\n",
        "                data.append(1)\n",
        "                I.append(row_idx)\n",
        "                J.append(edge_var_index + j * n_states + state)\n",
        "                #[row_idx, edge_var_index + j * n_states + state] = 1\n",
        "        row_idx += 1\n",
        "\n",
        "    coef = np.ravel(unaries)\n",
        "    # pairwise:\n",
        "    repeated_pairwise = edge_weights.ravel()\n",
        "    coef = np.hstack([coef, repeated_pairwise])\n",
        "    c = cvxopt.matrix(coef, tc='d')\n",
        "    # for positivity inequalities\n",
        "    G = cvxopt.spdiag(cvxopt.matrix(-np.ones(n_variables)))\n",
        "    #G = cvxopt.matrix(-np.eye(n_variables))\n",
        "    h = cvxopt.matrix(np.zeros(n_variables))  # for positivity inequalities\n",
        "    # unary and pairwise summation constratints\n",
        "    A = cvxopt.spmatrix(data, I, J)\n",
        "    assert(n_constraints == A.size[0])\n",
        "    b_ = np.zeros(A.size[0])  # zeros for pairwise summation constraints\n",
        "    b_[:n_nodes] = 1    # ones for unary summation constraints\n",
        "    b = cvxopt.matrix(b_)\n",
        "\n",
        "    # don't be verbose.\n",
        "    show_progress_backup = cvxopt.solvers.options.get('show_progress', False)\n",
        "    cvxopt.solvers.options['show_progress'] = False\n",
        "    result = cvxopt.solvers.lp(c, G, h, A, b)\n",
        "    cvxopt.solvers.options['show_progress'] = show_progress_backup\n",
        "\n",
        "    x = np.array(result['x'])\n",
        "    unary_variables = x[:n_nodes * n_states].reshape(n_nodes, n_states)\n",
        "    pairwise_variables = x[n_nodes * n_states:].reshape(n_edges, n_states ** 2)\n",
        "    assert((np.abs(unary_variables.sum(axis=1) - 1) < 1e-4).all())\n",
        "    assert((np.abs(pairwise_variables.sum(axis=1) - 1) < 1e-4).all())\n",
        "    return unary_variables, pairwise_variables, result['primal objective']\n",
        "\n",
        "def _validate_params(unary_potentials, pairwise_params, edges):\n",
        "    n_states = unary_potentials.shape[-1]\n",
        "    if pairwise_params.shape == (n_states, n_states):\n",
        "        # only one matrix given\n",
        "        pairwise_potentials = np.repeat(pairwise_params[np.newaxis, :, :],\n",
        "                                        edges.shape[0], axis=0)\n",
        "    else:\n",
        "        if pairwise_params.shape != (edges.shape[0], n_states, n_states):\n",
        "            raise ValueError(\"Expected pairwise_params either to \"\n",
        "                             \"be of shape n_states x n_states \"\n",
        "                             \"or n_edges x n_states x n_states, but\"\n",
        "                             \" got shape %s\" % repr(pairwise_params.shape))\n",
        "        pairwise_potentials = pairwise_params\n",
        "    return n_states, pairwise_potentials\n",
        "\n",
        "def crammer_singer_joint_feature(X, Y,out):\n",
        "    for i in range(X.shape[0]):\n",
        "        y = Y[i]\n",
        "        for j in range(X.shape[1]):\n",
        "            out[y, j] += X[i, j]\n",
        "def get_installed(method_filter=None):\n",
        "    if method_filter is None:\n",
        "        method_filter = [\"max-product\", 'ad3', 'ad3+', 'qpbo', 'ogm', 'lp']\n",
        "\n",
        "    installed = []\n",
        "    unary = np.zeros((1, 1))\n",
        "    pw = np.zeros((1, 1))\n",
        "    edges = np.empty((0, 2), dtype=np.int)\n",
        "    for method in method_filter:\n",
        "        try:\n",
        "            if method != 'ad3+':\n",
        "                inference_dispatch(unary, pw, edges, inference_method=method)\n",
        "            else:\n",
        "                inference_dispatch(unary, np.zeros((0,1,1))\n",
        "                                   , np.zeros((0,2), dtype=np.int)\n",
        "                                   , inference_method=method)\n",
        "            installed.append(method)\n",
        "        except ImportError:\n",
        "            pass\n",
        "    return installed\n",
        "def inference_dispatch(unary_potentials, pairwise_potentials, edges,\n",
        "                       inference_method, return_energy=False, **kwargs):\n",
        "    \"\"\"\n",
        "    Computes the maximizing assignment of a pairwise discrete energy function.\n",
        "    Wrapper function to dispatch between inference method by string.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array, shape (n_nodes, n_states)\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array, shape (n_states, n_states) \n",
        "                        or (n_states, n_states, n_edges).\n",
        "        Pairwise potentials of energy function.\n",
        "        If the first case, edge potentials are assumed to be the same for all \n",
        "        edges.\n",
        "        In the second case, the sequence needs to correspond to the edges.\n",
        "    edges : nd-array, shape (n_edges, 2)\n",
        "        Graph edges for pairwise potentials, given as pair of node indices. As\n",
        "        pairwise potentials are not assumed to be symmetric, the direction of\n",
        "        the edge matters.\n",
        "    inference_method : string\n",
        "        Possible choices currently are:\n",
        "            * 'qpbo' for QPBO alpha-expansion (fast but approximate).\n",
        "            * 'lp' for build-in lp relaxation via cvxopt (slow).\n",
        "            * 'ad3' for AD^3 subgradient based dual solution of LP.\n",
        "            * 'ogm' for OpenGM wrappers.\n",
        "            * 'max-product' for max-product message passing.\n",
        "            * 'unary' for using unary potentials only.\n",
        "        It is also possible to pass a tuple (string, dict) where the dict\n",
        "        contains additional keyword arguments, like\n",
        "        ``('ad3', {'branch_and_bound': True})``.\n",
        "    relaxed : bool (default=False)\n",
        "        Whether to return a relaxed solution (when appropriate)\n",
        "        or round to the nearest integer solution. Only used for 'lp' and 'ad3'\n",
        "        inference methods.\n",
        "    return_energy : bool (default=False)\n",
        "        Additionally return the energy of the returned solution (according to\n",
        "        the solver).  If relaxed=False, this is the energy of the relaxed, not\n",
        "        the rounded solution.\n",
        "    Returns\n",
        "    -------\n",
        "    labels : nd-array\n",
        "        Approximate (usually) MAP variable assignment.\n",
        "        If relaxed=True, this is a tuple of unary and pairwise \"marginals\"\n",
        "        from the LP relaxation.\n",
        "    \"\"\"\n",
        "    if isinstance(inference_method, tuple):\n",
        "        additional_kwargs = inference_method[1]\n",
        "        inference_method = inference_method[0]\n",
        "        # append additional_kwargs, but take care not to modify the dicts we\n",
        "        # got\n",
        "        kwargs = kwargs.copy()\n",
        "        kwargs.update(additional_kwargs)\n",
        "    if inference_method == \"qpbo\":\n",
        "        return inference_qpbo(unary_potentials, pairwise_potentials, edges,\n",
        "                              **kwargs)\n",
        "    elif inference_method == \"lp\":\n",
        "        return inference_lp(unary_potentials, pairwise_potentials, edges,\n",
        "                            return_energy=return_energy, **kwargs)\n",
        "    elif inference_method == \"ad3\":\n",
        "        return inference_ad3(unary_potentials, pairwise_potentials, edges,\n",
        "                             return_energy=return_energy, **kwargs)\n",
        "    elif inference_method == \"ad3+\":\n",
        "        return inference_ad3plus(unary_potentials, pairwise_potentials, edges,\n",
        "                             return_energy=return_energy, **kwargs)\n",
        "    elif inference_method == \"ogm\":\n",
        "        return inference_ogm(unary_potentials, pairwise_potentials, edges,\n",
        "                             return_energy=return_energy, **kwargs)\n",
        "    elif inference_method == \"unary\":\n",
        "        return inference_unaries(unary_potentials, pairwise_potentials, edges,\n",
        "                                 **kwargs)\n",
        "    elif inference_method == \"max-product\":\n",
        "        return inference_max_product(unary_potentials, pairwise_potentials,\n",
        "                                     edges, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"inference_method must be 'max-product', 'lp', 'ad3',\"\n",
        "                         \" 'qpbo' or 'ogm', got %s\" % inference_method)\n",
        "\n",
        "\n",
        "def inference_ogm(unary_potentials, pairwise_potentials, edges,\n",
        "                  return_energy=False, alg='dd', init=None,\n",
        "                  reserveNumFactorsPerVariable=2, **kwargs):\n",
        "    \"\"\"Inference with OpenGM backend.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array, shape (n_nodes, n_states)\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array, shape (n_states, n_states) \n",
        "                        or (n_states, n_states, n_edges).\n",
        "        Pairwise potentials of energy function.\n",
        "        If the first case, edge potentials are assumed to be the same for all \n",
        "        edges.\n",
        "        In the second case, the sequence needs to correspond to the edges.\n",
        "    edges : nd-array, shape (n_edges, 2)\n",
        "        Graph edges for pairwise potentials, given as pair of node indices. As\n",
        "        pairwise potentials are not assumed to be symmetric, the direction of\n",
        "        the edge matters.\n",
        "    alg : string\n",
        "        Possible choices currently are:\n",
        "            * 'bp' for Loopy Belief Propagation.\n",
        "            * 'dd' for Dual Decomposition via Subgradients.\n",
        "            * 'trws' for Vladimirs TRWs implementation.\n",
        "            * 'trw' for OGM  TRW.\n",
        "            * 'gibbs' for Gibbs sampling.\n",
        "            * 'lf' for Lazy Flipper\n",
        "            * 'fm' for Fusion Moves (alpha-expansion fusion)\n",
        "            * 'dyn' for Dynamic Programming (message passing in trees)\n",
        "            * 'gc' for Graph Cut\n",
        "            * 'alphaexp' for Alpha Expansion using Graph Cuts\n",
        "            * 'mqpbo' for multi-label qpbo\n",
        "    init : nd-array\n",
        "        Initial solution for starting inference (ignored by some algorithms).\n",
        "    reserveNumFactorsPerVariable :\n",
        "        reserve a certain number of factors for each variable can speed up\n",
        "        the building of a graphical model.\n",
        "        ( For a 2d grid with second order factors one should set this to 5\n",
        "         4 2-factors and 1 unary factor for most pixels )\n",
        "    Returns\n",
        "    -------\n",
        "    labels : nd-array\n",
        "        Approximate (usually) MAP variable assignment.\n",
        "    \"\"\"\n",
        "\n",
        "    import opengm\n",
        "    n_states, pairwise_potentials = \\\n",
        "        _validate_params(unary_potentials, pairwise_potentials, edges)\n",
        "    n_nodes = len(unary_potentials)\n",
        "\n",
        "    gm = opengm.gm(np.ones(n_nodes, dtype=opengm.label_type) * n_states)\n",
        "\n",
        "    nFactors = int(n_nodes + edges.shape[0])\n",
        "    gm.reserveFactors(nFactors)\n",
        "    gm.reserveFunctions(nFactors, 'explicit')\n",
        "\n",
        "    # all unaries as one numpy array\n",
        "    # (opengm's value_type == float64 but all types are accepted)\n",
        "    unaries = np.require(unary_potentials, dtype=opengm.value_type) * -1.0\n",
        "    # add all unart functions at once\n",
        "    fidUnaries = gm.addFunctions(unaries)\n",
        "    visUnaries = np.arange(n_nodes, dtype=opengm.label_type)\n",
        "    # add all unary factors at once\n",
        "    gm.addFactors(fidUnaries, visUnaries)\n",
        "\n",
        "    # add all pariwise functions at once\n",
        "    # - first axis of secondOrderFunctions iterates over the function)\n",
        "\n",
        "    secondOrderFunctions = -np.require(pairwise_potentials,\n",
        "                                       dtype=opengm.value_type)\n",
        "    fidSecondOrder = gm.addFunctions(secondOrderFunctions)\n",
        "    gm.addFactors(fidSecondOrder, edges.astype(np.uint64))\n",
        "\n",
        "    if alg == 'bp':\n",
        "        inference = opengm.inference.BeliefPropagation(gm)\n",
        "    elif alg == 'dd':\n",
        "        inference = opengm.inference.DualDecompositionSubgradient(gm)\n",
        "    elif alg == 'trws':\n",
        "        inference = opengm.inference.TrwsExternal(gm)\n",
        "    elif alg == 'trw':\n",
        "        inference = opengm.inference.TreeReweightedBp(gm)\n",
        "    elif alg == 'gibbs':\n",
        "        inference = opengm.inference.Gibbs(gm)\n",
        "    elif alg == 'lf':\n",
        "        inference = opengm.inference.LazyFlipper(gm)\n",
        "    elif alg == 'icm':\n",
        "        inference = opengm.inference.Icm(gm)\n",
        "    elif alg == 'dyn':\n",
        "        inference = opengm.inference.DynamicProgramming(gm)\n",
        "    elif alg == 'fm':\n",
        "        inference = opengm.inference.AlphaExpansionFusion(gm)\n",
        "    elif alg == 'gc':\n",
        "        inference = opengm.inference.GraphCut(gm)\n",
        "    elif alg == 'loc':\n",
        "        inference = opengm.inference.Loc(gm)\n",
        "    elif alg == 'mqpbo':\n",
        "        inference = opengm.inference.Mqpbo(gm)\n",
        "    elif alg == 'alphaexp':\n",
        "        inference = opengm.inference.AlphaExpansion(gm)\n",
        "    if init is not None:\n",
        "        inference.setStartingPoint(init)\n",
        "\n",
        "    inference.infer()\n",
        "    # we convert the result to int from unsigned int\n",
        "    # because otherwise we are sure to shoot ourself in the foot\n",
        "    res = inference.arg().astype(np.int)\n",
        "    if return_energy:\n",
        "        return res, gm.evaluate(res)\n",
        "    return res\n",
        "def inference_ad3(unary_potentials, pairwise_potentials, edges, relaxed=False,\n",
        "                  verbose=0, return_energy=False, branch_and_bound=False,\n",
        "                  inference_exception=None):\n",
        "    \"\"\"Inference with AD3 dual decomposition subgradient solver.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array, shape (n_nodes, n_states)\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array, shape (n_states, n_states) \n",
        "                        or (n_states, n_states, n_edges).\n",
        "        Pairwise potentials of energy function.\n",
        "        If the first case, edge potentials are assumed to be the same for all \n",
        "        edges.\n",
        "        In the second case, the sequence needs to correspond to the edges.\n",
        "    edges : nd-array, shape (n_edges, 2)\n",
        "        Graph edges for pairwise potentials, given as pair of node indices. As\n",
        "        pairwise potentials are not assumed to be symmetric, the direction of\n",
        "        the edge matters.\n",
        "    relaxed : bool (default=False)\n",
        "        Whether to return the relaxed solution (``True``) or round to the next\n",
        "        integer solution (``False``).\n",
        "    verbose : int (default=0)\n",
        "        Degree of verbosity for solver.\n",
        "    return_energy : bool (default=False)\n",
        "        Additionally return the energy of the returned solution (according to\n",
        "        the solver).  If relaxed=False, this is the energy of the relaxed, not\n",
        "        the rounded solution.\n",
        "    branch_and_bound : bool (default=False)\n",
        "        Whether to attempt to produce an integral solution using\n",
        "        branch-and-bound.\n",
        "    Returns\n",
        "    -------\n",
        "    labels : nd-array\n",
        "        Approximate (usually) MAP variable assignment.\n",
        "        If relaxed=False, this is a tuple of unary and edge 'marginals'.\n",
        "        \n",
        "    Code updated on Feb 2017 to deal with multiple node types, by JL Meunier\n",
        "    , for the EU READ project (grant agreement No 674943)\n",
        "    \n",
        "    \"\"\"\n",
        "    import ad3\n",
        "    bMultiType = isinstance(unary_potentials, list)\n",
        "    if bMultiType:\n",
        "        res = ad3.general_graph(unary_potentials, edges, pairwise_potentials\n",
        "                                , verbose=verbose\n",
        "                                , n_iterations=4000, exact=branch_and_bound)\n",
        "    else:\n",
        "        #usual code\n",
        "        n_states, pairwise_potentials = \\\n",
        "            _validate_params(unary_potentials, pairwise_potentials, edges)\n",
        "        unaries = unary_potentials.reshape(-1, n_states)\n",
        "        res = ad3.general_graph(unaries, edges, pairwise_potentials\n",
        "                                , verbose=verbose, n_iterations=4000\n",
        "                                , exact=branch_and_bound)\n",
        "        \n",
        "    unary_marginals, pairwise_marginals, energy, solver_status = res\n",
        "    if verbose:\n",
        "        print(solver_status)\n",
        "\n",
        "    if solver_status in [\"fractional\", \"unsolved\"] and relaxed:\n",
        "        if bMultiType:\n",
        "            y = (unary_marginals, pairwise_marginals)  #those two are lists\n",
        "        else:\n",
        "            #usual code\n",
        "            unary_marginals = unary_marginals.reshape(unary_potentials.shape)\n",
        "            y = (unary_marginals, pairwise_marginals)\n",
        "        #print solver_status, pairwise_marginals\n",
        "    else:\n",
        "        if bMultiType:\n",
        "            #we now get a list of unary marginals\n",
        "            if inference_exception and solver_status in [\"fractional\"\n",
        "                                                         , \"unsolved\"]:\n",
        "                raise InferenceException(solver_status)\n",
        "            ly = list()\n",
        "            _cum_n_states = 0\n",
        "            for unary_marg in unary_marginals:\n",
        "                ly.append( _cum_n_states + np.argmax(unary_marg, axis=-1) )\n",
        "                # number of states for that type\n",
        "                _cum_n_states += unary_marg.shape[1]  \n",
        "            y = np.hstack(ly)            \n",
        "        else:\n",
        "            #usual code\n",
        "            y = np.argmax(unary_marginals, axis=-1)\n",
        "        \n",
        "    if return_energy:\n",
        "        return y, -energy\n",
        "    return y\n",
        "\n",
        "\n",
        "def inference_ad3plus(l_unary_potentials, l_pairwise_potentials, l_edges\n",
        "                      , relaxed=False\n",
        "                      , verbose=0, return_energy=False, branch_and_bound=False\n",
        "                      , constraints=None, inference_exception=None):\n",
        "    \"\"\"\n",
        "    Inference with AD3 dual decomposition subgradient solver.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array, shape (n_nodes, n_states)\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array, shape (n_states, n_states) \n",
        "                        or (n_states, n_states, n_edges).\n",
        "        Pairwise potentials of energy function.\n",
        "        If the first case, edge potentials are assumed to be the same for all \n",
        "        edges.\n",
        "        In the second case, the sequence needs to correspond to the edges.\n",
        "    edges : nd-array, shape (n_edges, 2)\n",
        "        Graph edges for pairwise potentials, given as pair of node indices. As\n",
        "        pairwise potentials are not assumed to be symmetric, the direction of\n",
        "        the edge matters.\n",
        "    relaxed : bool (default=False)\n",
        "        Whether to return the relaxed solution (``True``) or round to the next\n",
        "        integer solution (``False``).\n",
        "    verbose : int (default=0)\n",
        "        Degree of verbosity for solver.\n",
        "    return_energy : bool (default=False)\n",
        "        Additionally return the energy of the returned solution (according to\n",
        "        the solver).  If relaxed=False, this is the energy of the relaxed, not\n",
        "        the rounded solution.\n",
        "    branch_and_bound : bool (default=False)\n",
        "        Whether to attempt to produce an integral solution using\n",
        "        branch-and-bound.\n",
        "    constraints : list of logical constraints or None (default:=None)\n",
        "        A logical constraint is tuple like \n",
        "            ( <operator>, <unaries>, <states>, <negated> )\n",
        "        where:\n",
        "        - operator is one of:\n",
        "             'XOR' 'XOROUT' 'ATMOSTONE' 'OR' 'OROUT' 'ANDOUT' 'IMPLY'\n",
        "        - unaries is a list of the index of each unary involved in this \n",
        "        constraint\n",
        "        - states is a list of unary states (class), 1 per involved unary. If the\n",
        "         states are all the same, you can pass it directly as a scalar value.\n",
        "        - negated is a list of boolean indicating if the unary must be negated.\n",
        "        Again, if all values are the same, pass a single boolean value instead\n",
        "        of a list \n",
        "        \n",
        "        NOTE: this hard logic constraint mechanism has been developed for the \n",
        "        EU project READ, by JL Meunier (Xerox), in November 2016.\n",
        "        The READ project has received funding from the European Union's Horizon\n",
        "        2020 research and innovation programme under grant agreement No 674943.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    labels : nd-array\n",
        "        Approximate (usually) MAP variable assignment.\n",
        "        If relaxed=False, this is a tuple of unary and edge 'marginals'.\n",
        "    \"\"\"\n",
        "    import ad3\n",
        "#     n_states, pairwise_potentials = \\\n",
        "#         _validate_params(unary_potentials, pairwise_potentials, edges)\n",
        "#     unaries = unary_potentials.reshape(-1, n_states)\n",
        "    bMultiType = isinstance(l_unary_potentials, list)\n",
        "\n",
        "    res = ad3.general_constrained_graph(l_unary_potentials, l_edges\n",
        "                                        , l_pairwise_potentials, constraints\n",
        "                                        , verbose=verbose\n",
        "                                        , n_iterations=4000\n",
        "                                        , exact=branch_and_bound)\n",
        "    \n",
        "    l_unary_marginals, l_pairwise_marginals, energy, solver_status = res\n",
        "    if verbose:\n",
        "        print(solver_status)\n",
        "\n",
        "    if relaxed and solver_status in [\"fractional\", \"unsolved\"]:\n",
        "        y = (l_unary_marginals, l_pairwise_marginals)\n",
        "    else:\n",
        "        if inference_exception and solver_status in [\"fractional\", \"unsolved\"]:\n",
        "            raise InferenceException(solver_status)\n",
        "        if bMultiType:\n",
        "            #we now get a list of unary marginals\n",
        "            ly = list()\n",
        "            _cum_n_states = 0\n",
        "            for unary_marg in l_unary_marginals:\n",
        "                ly.append( _cum_n_states + np.argmax(unary_marg, axis=-1) )\n",
        "                #number of states for that type\n",
        "                _cum_n_states += unary_marg.shape[1] \n",
        "            y = np.hstack(ly)\n",
        "            # when we will simplify y: \n",
        "            #y = [_cum_n_statesnp.argmax(unary_marg, axis=-1) for unary_marg \n",
        "            #  in l_unary_marginals]\n",
        "        else:\n",
        "            y = np.argmax(l_unary_marginals, axis=-1)\n",
        "\n",
        "    if return_energy:\n",
        "        return y, -energy\n",
        "    return y\n",
        "def inference_max_product(unary_potentials, pairwise_potentials, edges,\n",
        "                          max_iter=30, damping=0.5, tol=1e-5, relaxed=None):\n",
        "    \"\"\"Max-product inference.\n",
        "    In case the edges specify a tree, dynamic programming is used\n",
        "    producing a result in only a single pass.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array\n",
        "        Pairwise potentials of energy function.\n",
        "    edges : nd-array\n",
        "        Edges of energy function.\n",
        "    max_iter : int (default=10)\n",
        "        Maximum number of iterations. Ignored if graph is a tree.\n",
        "    damping : float (default=.5)\n",
        "        Daming of messages in loopy message passing.\n",
        "        Ignored if graph is a tree.\n",
        "    tol : float (default=1e-5)\n",
        "        Stopping tollerance for loopy message passing.\n",
        "    \"\"\"\n",
        "    from ._viterbi import viterbi\n",
        "    n_states, pairwise_potentials = \\\n",
        "        _validate_params(unary_potentials, pairwise_potentials, edges)\n",
        "    if is_chain(edges=edges, n_vertices=len(unary_potentials)):\n",
        "        y = viterbi(unary_potentials.astype(np.float).copy(),\n",
        "                    # sad second copy b/c numpy 1.6\n",
        "                    np.array(pairwise_potentials, dtype=np.float))\n",
        "    elif is_forest(edges=edges, n_vertices=len(unary_potentials)):\n",
        "        y = tree_max_product(unary_potentials, pairwise_potentials, edges)\n",
        "    else:\n",
        "        y = iterative_max_product(unary_potentials, pairwise_potentials, edges,\n",
        "                                  max_iter=max_iter, damping=damping)\n",
        "    return y\n",
        "\n",
        "\n",
        "def tree_max_product(unary_potentials, pairwise_potentials, edges):\n",
        "    n_vertices, n_states = unary_potentials.shape\n",
        "    parents = -np.ones(n_vertices, dtype=np.int)\n",
        "    visited = np.zeros(n_vertices, dtype=np.bool)\n",
        "    neighbors = [[] for i in range(n_vertices)]\n",
        "    pairwise_weights = [[] for i in range(n_vertices)]\n",
        "    for pw, edge in zip(pairwise_potentials, edges):\n",
        "        neighbors[edge[0]].append(edge[1])\n",
        "        pairwise_weights[edge[0]].append(pw)\n",
        "        neighbors[edge[1]].append(edge[0])\n",
        "        pairwise_weights[edge[1]].append(pw.T)\n",
        "\n",
        "    messages_forward = np.zeros((n_vertices, n_states))\n",
        "    messages_backward = np.zeros((n_vertices, n_states))\n",
        "    pw_forward = np.zeros((n_vertices, n_states, n_states))\n",
        "    # build a breadth first search of the tree\n",
        "    traversal = []\n",
        "    lonely = 0\n",
        "    while lonely < n_vertices:\n",
        "        for i in range(lonely, n_vertices):\n",
        "            if not visited[i]:\n",
        "                queue = [i]\n",
        "                lonely = i + 1\n",
        "                visited[i] = True\n",
        "                break\n",
        "            lonely = n_vertices\n",
        "\n",
        "        while queue:\n",
        "            node = queue.pop(0)\n",
        "            traversal.append(node)\n",
        "            for pw, neighbor in zip(pairwise_weights[node], neighbors[node]):\n",
        "                if not visited[neighbor]:\n",
        "                    parents[neighbor] = node\n",
        "                    queue.append(neighbor)\n",
        "                    visited[neighbor] = True\n",
        "                    pw_forward[neighbor] = pw\n",
        "\n",
        "                elif not parents[node] == neighbor:\n",
        "                    raise ValueError(\"Graph not a tree\")\n",
        "    # messages from leaves to root\n",
        "    for node in traversal[::-1]:\n",
        "        parent = parents[node]\n",
        "        if parent != -1:\n",
        "            message = np.max(messages_backward[node] + unary_potentials[node] +\n",
        "                             pw_forward[node], axis=1)\n",
        "            message -= message.max()\n",
        "            messages_backward[parent] += message\n",
        "    # messages from root back to leaves\n",
        "    for node in traversal:\n",
        "        parent = parents[node]\n",
        "        if parent != -1:\n",
        "            message = messages_forward[parent] + unary_potentials[parent] + pw_forward[node].T\n",
        "            # leaves to root messages from other children\n",
        "            message += messages_backward[parent] - np.max(messages_backward[node]\n",
        "                                                          + unary_potentials[node]\n",
        "                                                          + pw_forward[node], axis=1)\n",
        "            message = message.max(axis=1)\n",
        "            message -= message.max()\n",
        "            messages_forward[node] += message\n",
        "\n",
        "    return np.argmax(unary_potentials + messages_forward + messages_backward, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bchnyP-xe853"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def unwrap_pairwise(y):\n",
        "    \"\"\"given a y that may contain pairwise marginals, yield plain y.\"\"\"\n",
        "    if isinstance(y, tuple):\n",
        "        return y[0]\n",
        "    return y\n",
        "\n",
        "\n",
        "def expand_sym(sym_compressed):\n",
        "    \"\"\"Expand compressed symmetric matrix to full square matrix.\n",
        "\n",
        "    Similar to scipy.spatial.squareform, but also contains the\n",
        "    diagonal.\n",
        "    \"\"\"\n",
        "    length = sym_compressed.size\n",
        "    size = int(np.sqrt(2 * length + 0.25) - 1 / 2.)\n",
        "    sym = np.zeros((size, size))\n",
        "    sym[np.tri(size, dtype=np.bool)] = sym_compressed\n",
        "    return (sym + sym.T - np.diag(np.diag(sym)))\n",
        "\n",
        "\n",
        "def compress_sym(sym_expanded, make_symmetric=True):\n",
        "    \"\"\"Compress symmetric matrix to a vector.\n",
        "\n",
        "    Similar to scipy.spatial.squareform, but also contains the\n",
        "    diagonal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sym_expanded : nd-array, shape (size, size)\n",
        "        Input matrix to compress.\n",
        "\n",
        "    make_symmetric : bool (default=True)\n",
        "        Whether to symmetrize the input matrix before compressing.\n",
        "        It is made symmetric by using\n",
        "        ``sym_expanded + sym_expanded.T - np.diag(np.diag(sym_expanded))``\n",
        "        This makes sense if only one of the two entries was non-zero before.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    size = sym_expanded.shape[0]\n",
        "    if make_symmetric:\n",
        "        sym_expanded = (sym_expanded + sym_expanded.T -\n",
        "                        np.diag(np.diag(sym_expanded)))\n",
        "    return sym_expanded[np.tri(size, dtype=np.bool)]\n",
        "\n",
        "\n",
        "## global functions for easy parallelization\n",
        "def find_constraint(model, x, y, w, y_hat=None, relaxed=True,\n",
        "                    compute_difference=True):\n",
        "    \"\"\"Find most violated constraint, or, given y_hat,\n",
        "    find slack and djoint_feature for this constraing.\n",
        "\n",
        "    As for finding the most violated constraint, it is enough to compute\n",
        "    joint_feature(x, y_hat), not djoint_feature, we can optionally skip\n",
        "    computing joint_feature(x, y) using compute_differences=False\n",
        "    \"\"\"\n",
        "\n",
        "    if y_hat is None:\n",
        "        y_hat = model.loss_augmented_inference(x, y, w, relaxed=relaxed)\n",
        "    joint_feature = model.joint_feature\n",
        "    if getattr(model, 'rescale_C', False):\n",
        "        delta_joint_feature = -joint_feature(x, y_hat, y)\n",
        "    else:\n",
        "        delta_joint_feature = -joint_feature(x, y_hat)\n",
        "    if compute_difference:\n",
        "        if getattr(model, 'rescale_C', False):\n",
        "            delta_joint_feature += joint_feature(x, y, y)\n",
        "        else:\n",
        "            delta_joint_feature += joint_feature(x, y)\n",
        "\n",
        "    if isinstance(y_hat, tuple):\n",
        "        # continuous label\n",
        "        loss = model.continuous_loss(y, y_hat[0])\n",
        "    else:\n",
        "        loss = model.loss(y, y_hat)\n",
        "    slack = max(loss - np.dot(w, delta_joint_feature), 0)\n",
        "    return y_hat, delta_joint_feature, slack, loss\n",
        "\n",
        "\n",
        "def find_constraint_latent(model, x, y, w, relaxed=True):\n",
        "    \"\"\"Find most violated constraint.\n",
        "\n",
        "    As for finding the most violated constraint, it is enough to compute\n",
        "    joint_feature(x, y_hat), not djoint_feature, we can optionally skip\n",
        "    computing joint_feature(x, y) using compute_differences=False\n",
        "    \"\"\"\n",
        "    h = model.latent(x, y, w)\n",
        "    h_hat = model.loss_augmented_inference(x, h, w, relaxed=relaxed)\n",
        "    joint_feature = model.joint_feature\n",
        "    delta_joint_feature = joint_feature(x, h) - joint_feature(x, h_hat)\n",
        "\n",
        "    loss = model.loss(y, h_hat)\n",
        "    slack = max(loss - np.dot(w, delta_joint_feature), 0)\n",
        "    return h_hat, delta_joint_feature, slack, loss\n",
        "\n",
        "\n",
        "def inference(model, x, w, constraints=None):\n",
        "    if constraints:\n",
        "        return model.inference(x, w, constraints=constraints)\n",
        "    else:\n",
        "        return model.inference(x, w)\n",
        "\n",
        "\n",
        "def loss_augmented_inference(model, x, y, w, relaxed=True):\n",
        "    return model.loss_augmented_inference(x, y, w, relaxed=relaxed)\n",
        "\n",
        "\n",
        "# easy debugging\n",
        "def objective_primal(model, w, X, Y, C, variant='n_slack', n_jobs=1):\n",
        "    objective = 0\n",
        "    constraints = Parallel(\n",
        "        n_jobs=n_jobs)(delayed(find_constraint)(\n",
        "            model, x, y, w)\n",
        "            for x, y in zip(X, Y))\n",
        "    slacks = list(zip(*constraints))[2]\n",
        "\n",
        "    if variant == 'n_slack':\n",
        "        slacks = np.maximum(slacks, 0)\n",
        "\n",
        "    objective = max(np.sum(slacks), 0) * C + np.sum(w ** 2) / 2.\n",
        "    return objective\n",
        "\n",
        "\n",
        "def exhaustive_loss_augmented_inference(model, x, y, w):\n",
        "    size = y.size\n",
        "    best_y = None\n",
        "    best_energy = np.inf\n",
        "    for y_hat in itertools.product(range(model.n_states), repeat=size):\n",
        "        y_hat = np.array(y_hat).reshape(y.shape)\n",
        "        #print(\"trying %s\" % repr(y_hat))\n",
        "        joint_feature = model.joint_feature(x, y_hat)\n",
        "        energy = -model.loss(y, y_hat) - np.dot(w, joint_feature)\n",
        "        if energy < best_energy:\n",
        "            best_energy = energy\n",
        "            best_y = y_hat\n",
        "    return best_y\n",
        "\n",
        "\n",
        "def exhaustive_inference(model, x, w):\n",
        "    # hack to get the grid shape of x\n",
        "    if isinstance(x, np.ndarray):\n",
        "        feats = x\n",
        "    else:\n",
        "        feats = model._get_features(x)\n",
        "    size = np.prod(feats.shape[:-1])\n",
        "    best_y = None\n",
        "    best_energy = np.inf\n",
        "    for y_hat in itertools.product(range(model.n_states), repeat=size):\n",
        "        y_hat = np.array(y_hat).reshape(feats.shape[:-1])\n",
        "        #print(\"trying %s\" % repr(y_hat))\n",
        "        joint_feature = model.joint_feature(x, y_hat)\n",
        "        energy = -np.dot(w, joint_feature)\n",
        "        if energy < best_energy:\n",
        "            best_energy = energy\n",
        "            best_y = y_hat\n",
        "    return best_y\n",
        "def inference_lp(unary_potentials, pairwise_potentials, edges, relaxed=False,\n",
        "                 return_energy=False, **kwargs):\n",
        "    \"\"\"Inference with build-in LP solver using cvxopt backend.\n",
        "    Parameters\n",
        "    ----------\n",
        "    unary_potentials : nd-array, shape (n_nodes, n_states)\n",
        "        Unary potentials of energy function.\n",
        "    pairwise_potentials : nd-array, shape (n_states, n_states) \n",
        "                        or (n_states, n_states, n_edges).\n",
        "        Pairwise potentials of energy function.\n",
        "        If the first case, edge potentials are assumed to be the same for all \n",
        "        edges.\n",
        "        In the second case, the sequence needs to correspond to the edges.\n",
        "    edges : nd-array, shape (n_edges, 2)\n",
        "        Graph edges for pairwise potentials, given as pair of node indices. As\n",
        "        pairwise potentials are not assumed to be symmetric, the direction of\n",
        "        the edge matters.\n",
        "    relaxed : bool (default=False)\n",
        "        Whether to return the relaxed solution (``True``) or round to the next\n",
        "        integer solution (``False``).\n",
        "    return_energy : bool (default=False)\n",
        "        Additionally return the energy of the returned solution (according to\n",
        "        the solver).  If relaxed=False, this is the energy of the relaxed, not\n",
        "        the rounded solution.\n",
        "    Returns\n",
        "    -------\n",
        "    labels : nd-array\n",
        "        Approximate (usually) MAP variable assignment.\n",
        "        If relaxed=False, this is a tuple of unary and edge 'marginals'.\n",
        "    \"\"\"\n",
        "    shape_org = unary_potentials.shape[:-1]\n",
        "    n_states, pairwise_potentials = \\\n",
        "        _validate_params(unary_potentials, pairwise_potentials, edges)\n",
        "\n",
        "    unaries = unary_potentials.reshape(-1, n_states)\n",
        "    res = lp_general_graph(-unaries, edges, -pairwise_potentials)\n",
        "    unary_marginals, pairwise_marginals, energy = res\n",
        "    if relaxed:\n",
        "        unary_marginals = unary_marginals.reshape(unary_potentials.shape)\n",
        "        y = (unary_marginals, pairwise_marginals)\n",
        "    else:\n",
        "        y = np.argmax(unary_marginals, axis=-1)\n",
        "        y = y.reshape(shape_org)\n",
        "    if return_energy:\n",
        "        return y, energy\n",
        "    return y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0zii-7re854"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "\n",
        "class BaseSSVM(BaseEstimator):\n",
        "    \"\"\"ABC that implements common functionality.\"\"\"\n",
        "    def __init__(self, model, max_iter=100, C=1.0, verbose=0,\n",
        "                 n_jobs=1, show_loss_every=0, logger=None):\n",
        "        self.model = model\n",
        "        self.max_iter = max_iter\n",
        "        self.C = C\n",
        "        self.verbose = verbose\n",
        "        self.show_loss_every = show_loss_every\n",
        "        self.n_jobs = n_jobs\n",
        "        self.logger = logger\n",
        "\n",
        "    def predict(self, X, constraints=None):\n",
        "        \"\"\"Predict output on examples in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : iterable\n",
        "            Traing instances. Contains the structured input objects.\n",
        "\n",
        "        constraints : None or a list of hard logic constraints\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Y_pred : list\n",
        "            List of inference results for X using the learned parameters.\n",
        "\n",
        "        \"\"\"\n",
        "        verbose = max(0, self.verbose - 3)\n",
        "        if self.n_jobs != 1:\n",
        "            if constraints:\n",
        "                prediction = Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n",
        "                    delayed(inference)(self.model, x, self.w, constraints=c)\n",
        "                    for x, c in zip(X, constraints))\n",
        "            else:\n",
        "                prediction = Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n",
        "                    delayed(inference)(self.model, x, self.w) for x in X)\n",
        "            return prediction\n",
        "        else:\n",
        "            if hasattr(self.model, 'batch_inference'):\n",
        "                if constraints:\n",
        "                    return self.model.batch_inference(X, self.w,\n",
        "                                                      constraints=constraints)\n",
        "                else:\n",
        "                    return self.model.batch_inference(X, self.w)\n",
        "            if constraints:\n",
        "                return [self.model.inference(x, self.w, constraints=c)\n",
        "                        for x, c in zip(X, constraints)]\n",
        "            return [self.model.inference(x, self.w) for x in X]\n",
        "\n",
        "    def score(self, X, Y):\n",
        "        \"\"\"Compute score as 1 - loss over whole data set.\n",
        "\n",
        "        Returns the average accuracy (in terms of model.loss)\n",
        "        over X and Y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : iterable\n",
        "            Evaluation data.\n",
        "\n",
        "        Y : iterable\n",
        "            True labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        score : float\n",
        "            Average of 1 - loss over training examples.\n",
        "        \"\"\"\n",
        "        if hasattr(self.model, 'batch_loss'):\n",
        "            losses = self.model.batch_loss(Y, self.predict(X))\n",
        "        else:\n",
        "            losses = [self.model.loss(y, y_pred)\n",
        "                      for y, y_pred in zip(Y, self.predict(X))]\n",
        "        max_losses = [self.model.max_loss(y) for y in Y]\n",
        "        return 1. - np.sum(losses) / float(np.sum(max_losses))\n",
        "\n",
        "    def _compute_training_loss(self, X, Y, iteration):\n",
        "        # optionally compute training loss for output / training curve\n",
        "        if (self.show_loss_every != 0\n",
        "                and not iteration % self.show_loss_every):\n",
        "            if not hasattr(self, 'loss_curve_'):\n",
        "                self.loss_curve_ = []\n",
        "            display_loss = 1 - self.score(X, Y)\n",
        "            if self.verbose > 0:\n",
        "                print(\"current loss: %f\" % (display_loss))\n",
        "            self.loss_curve_.append(display_loss)\n",
        "\n",
        "    def _objective(self, X, Y):\n",
        "        if type(self).__name__ == 'OneSlackSSVM':\n",
        "            variant = 'one_slack'\n",
        "        else:\n",
        "            variant = 'n_slack'\n",
        "        return objective_primal(self.model, self.w, X, Y, self.C,\n",
        "                                variant=variant, n_jobs=self.n_jobs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4RtINpje855"
      },
      "outputs": [],
      "source": [
        "class StructuredModel(object):\n",
        "    \"\"\"Interface definition for Structured Learners.\n",
        "\n",
        "    This class defines what is necessary to use the structured svm.\n",
        "    You have to implement at least joint_feature and inference.\n",
        "    \"\"\"\n",
        "    def __repr__(self):\n",
        "        return (\"%s, size_joint_feature: %d\"\n",
        "                % (type(self).__name__, self.size_joint_feature))\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the model.\n",
        "        Needs to set self.size_joint_feature, the dimensionality of the joint\n",
        "        features for an instance with labeling (x, y).\n",
        "        \"\"\"\n",
        "        self.size_joint_feature = None\n",
        "\n",
        "    def _check_size_w(self, w):\n",
        "        if w.shape != (self.size_joint_feature,):\n",
        "            raise ValueError(\"Got w of wrong shape. Expected %s, got %s\" %\n",
        "                             (self.size_joint_feature, w.shape))\n",
        "\n",
        "    def initialize(self, X, Y):\n",
        "        # set any data-specific parameters in the model\n",
        "        pass\n",
        "\n",
        "    def joint_feature(self, x, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def batch_joint_feature(self, X, Y, Y_true=None):\n",
        "        joint_feature_ = np.zeros(self.size_joint_feature)\n",
        "        if getattr(self, 'rescale_C', False):\n",
        "            for x, y, y_true in zip(X, Y, Y_true):\n",
        "                joint_feature_ += self.joint_feature(x, y, y_true)\n",
        "        else:\n",
        "            for x, y in zip(X, Y):\n",
        "                joint_feature_ += self.joint_feature(x, y)\n",
        "        return joint_feature_\n",
        "\n",
        "    def _loss_augmented_djoint_feature(self, x, y, y_hat, w):\n",
        "        # debugging only!\n",
        "        x_loss_augmented = self.loss_augment(x, y, w)\n",
        "        return (self.joint_feature(x_loss_augmented, y)\n",
        "                - self.joint_feature(x_loss_augmented, y_hat))\n",
        "\n",
        "    def inference(self, x, w, relaxed=None, constraints=None):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def batch_inference(self, X, w, relaxed=None, constraints=None):\n",
        "        # default implementation of batch inference\n",
        "        if constraints:\n",
        "            return [self.inference(x, w, relaxed=relaxed, constraints=c)\n",
        "                    for x, c in zip(X, constraints)]\n",
        "        return [self.inference(x, w, relaxed=relaxed)\n",
        "                for x in X]\n",
        "\n",
        "    def loss(self, y, y_hat):\n",
        "        # hamming loss:\n",
        "        if isinstance(y_hat, tuple):\n",
        "            return self.continuous_loss(y, y_hat[0])\n",
        "        if hasattr(self, 'class_weight'):\n",
        "            return np.sum(self.class_weight[y] * (y != y_hat))\n",
        "        return np.sum(y != y_hat)\n",
        "\n",
        "    def batch_loss(self, Y, Y_hat):\n",
        "        # default implementation of batch loss\n",
        "        return [self.loss(y, y_hat) for y, y_hat in zip(Y, Y_hat)]\n",
        "\n",
        "    def max_loss(self, y):\n",
        "        # maximum possible los on y for macro averages\n",
        "        if hasattr(self, 'class_weight'):\n",
        "            return np.sum(self.class_weight[y])\n",
        "        return y.size\n",
        "\n",
        "    def continuous_loss(self, y, y_hat):\n",
        "        # continuous version of the loss\n",
        "        # y is the result of linear programming\n",
        "        if y.ndim == 2:\n",
        "            raise ValueError(\"FIXME!\")\n",
        "        gx = np.indices(y.shape)\n",
        "\n",
        "        # all entries minus correct ones\n",
        "        result = 1 - y_hat[gx, y]\n",
        "        if hasattr(self, 'class_weight'):\n",
        "            return np.sum(self.class_weight[y] * result)\n",
        "        return np.sum(result)\n",
        "\n",
        "    def loss_augmented_inference(self, x, y, w, relaxed=None):\n",
        "        print(\"FALLBACK no loss augmented inference found\")\n",
        "        return self.inference(x, w)\n",
        "\n",
        "    def batch_loss_augmented_inference(self, X, Y, w, relaxed=None):\n",
        "        # default implementation of batch loss augmented inference\n",
        "        return [self.loss_augmented_inference(x, y, w, relaxed=relaxed)\n",
        "                for x, y in zip(X, Y)]\n",
        "\n",
        "    def _set_class_weight(self):\n",
        "        if not hasattr(self, 'size_joint_feature'):\n",
        "            # we are not initialized yet\n",
        "            return\n",
        "\n",
        "        if hasattr(self, 'n_labels'):\n",
        "            n_things = self.n_labels\n",
        "        else:\n",
        "            n_things = self.n_states\n",
        "\n",
        "        if self.class_weight is not None:\n",
        "\n",
        "            if len(self.class_weight) != n_things:\n",
        "                raise ValueError(\"class_weight must have length n_states or\"\n",
        "                                 \" be None\")\n",
        "            self.class_weight = np.array(self.class_weight)\n",
        "            self.uniform_class_weight = False\n",
        "        else:\n",
        "            self.class_weight = np.ones(n_things)\n",
        "            self.uniform_class_weight = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D4I2tM4e856"
      },
      "outputs": [],
      "source": [
        "\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import cvxopt\n",
        "import cvxopt.solvers\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NoConstraint(Exception):\n",
        "    # raised if we can not construct a constraint from cache\n",
        "    pass\n",
        "\n",
        "\n",
        "class OneSlackSSVM(BaseSSVM):\n",
        "    \"\"\"Structured SVM solver for the 1-slack QP with l1 slack penalty.\n",
        "\n",
        "    Implements margin rescaled structural SVM using\n",
        "    the 1-slack formulation and cutting plane method, solved using CVXOPT.\n",
        "    The optimization is restarted in each iteration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : StructuredModel\n",
        "        Object containing the model structure. Has to implement\n",
        "        `loss`, `inference` and `loss_augmented_inference`.\n",
        "\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of passes over dataset to find constraints.\n",
        "\n",
        "    C : float, default=1\n",
        "        Regularization parameter.\n",
        "\n",
        "    check_constraints : bool\n",
        "        Whether to check if the new \"most violated constraint\" is\n",
        "        more violated than previous constraints. Helpful for stopping\n",
        "        and debugging, but costly.\n",
        "\n",
        "    verbose : int\n",
        "        Verbosity.\n",
        "\n",
        "    negativity_constraint : list of ints\n",
        "        Indices of parmeters that are constraint to be negative.\n",
        "        This is useful for learning submodular CRFs (inference is formulated\n",
        "        as maximization in SSVMs, flipping some signs).\n",
        "\n",
        "    break_on_bad : bool default=False\n",
        "        Whether to break (start debug mode) when inference was approximate.\n",
        "\n",
        "    n_jobs : int, default=1\n",
        "        Number of parallel jobs for inference. -1 means as many as cpus.\n",
        "\n",
        "    show_loss_every : int, default=0\n",
        "        Controlls how often the hamming loss is computed (for monitoring\n",
        "        purposes). Zero means never, otherwise it will be computed very\n",
        "        show_loss_every'th epoch.\n",
        "\n",
        "    tol : float, default=1e-3\n",
        "        Convergence tolerance. If dual objective decreases less than tol,\n",
        "        learning is stopped. The default corresponds to ignoring the behavior\n",
        "        of the dual objective and stop only if no more constraints can be\n",
        "        found.\n",
        "\n",
        "    inference_cache : int, default=0\n",
        "        How many results of loss_augmented_inference to cache per sample.\n",
        "        If > 0 the most violating of the cached examples will be used to\n",
        "        construct a global constraint. Only if this constraint is not violated,\n",
        "        inference will be run again. This parameter poses a memory /\n",
        "        computation tradeoff. Storing more constraints might lead to RAM being\n",
        "        exhausted. Using inference_cache > 0 is only advisable if computation\n",
        "        time is dominated by inference.\n",
        "\n",
        "    cache_tol : float, None or 'auto' default='auto'\n",
        "        Tolerance when to reject a constraint from cache (and do inference).\n",
        "        If None, ``tol`` will be used. Higher values might lead to faster\n",
        "        learning. 'auto' uses a heuristic to determine the cache tolerance\n",
        "        based on the duality gap, as described in [3].\n",
        "\n",
        "    inactive_threshold : float, default=1e-5\n",
        "        Threshold for dual variable of a constraint to be considered inactive.\n",
        "\n",
        "    inactive_window : float, default=50\n",
        "        Window for measuring inactivity. If a constraint is inactive for\n",
        "        ``inactive_window`` iterations, it will be pruned from the QP.\n",
        "        If set to 0, no constraints will be removed.\n",
        "\n",
        "    switch_to : None or string, default=None\n",
        "        Switch to the given inference method if the previous method does not\n",
        "        find any more constraints.\n",
        "\n",
        "    logger : logger object, default=None\n",
        "        Pystruct logger for storing the model or extracting additional\n",
        "        information.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    w : nd-array, shape=(model.size_joint_feature,)\n",
        "        The learned weights of the SVM.\n",
        "\n",
        "    old_solution : dict\n",
        "        The last solution found by the qp solver.\n",
        "\n",
        "    ``loss_curve_`` : list of float\n",
        "        List of loss values if show_loss_every > 0.\n",
        "\n",
        "    ``objective_curve_`` : list of float\n",
        "       Cutting plane objective after each pass through the dataset.\n",
        "\n",
        "    ``primal_objective_curve_`` : list of float\n",
        "        Primal objective after each pass through the dataset.\n",
        "\n",
        "    ``timestamps_`` : list of int\n",
        "       Total training time stored before each iteration.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    [1] Thorsten Joachims, and Thomas Finley and Chun-Nam John Yu:\n",
        "        Cutting-plane training of structural SVMs, JMLR 2009\n",
        "\n",
        "    [2] Andreas Mueller: Methods for Learning Structured Prediction in\n",
        "        Semantic Segmentation of Natural Images, PhD Thesis.  2014\n",
        "\n",
        "    [3] Andreas Mueller and Sven Behnke: Learning a Loopy Model For Semantic\n",
        "        Segmentation Exactly, VISAPP 2014\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, max_iter=10000, C=1.0, check_constraints=False,\n",
        "                 verbose=0, negativity_constraint=None, n_jobs=1,\n",
        "                 break_on_bad=False, show_loss_every=0, tol=1e-3,\n",
        "                 inference_cache=0, inactive_threshold=1e-5,\n",
        "                 inactive_window=50, logger=None, cache_tol='auto',\n",
        "                 switch_to=None):\n",
        "\n",
        "        BaseSSVM.__init__(self, model, max_iter, C, verbose=verbose,\n",
        "                          n_jobs=n_jobs, show_loss_every=show_loss_every,\n",
        "                          logger=logger)\n",
        "\n",
        "        self.negativity_constraint = negativity_constraint\n",
        "        self.check_constraints = check_constraints\n",
        "        self.break_on_bad = break_on_bad\n",
        "        self.tol = tol\n",
        "        self.cache_tol = cache_tol\n",
        "        self.inference_cache = inference_cache\n",
        "        self.inactive_threshold = inactive_threshold\n",
        "        self.inactive_window = inactive_window\n",
        "        self.switch_to = switch_to\n",
        "\n",
        "    def _solve_1_slack_qp(self, constraints, n_samples):\n",
        "        C = np.float(self.C) * n_samples  # this is how libsvm/svmstruct do it\n",
        "        joint_features = [c[0] for c in constraints]\n",
        "        losses = [c[1] for c in constraints]\n",
        "\n",
        "        joint_feature_matrix = np.vstack(joint_features)\n",
        "        n_constraints = len(joint_features)\n",
        "        P = cvxopt.matrix(np.dot(joint_feature_matrix, joint_feature_matrix.T))\n",
        "        # q contains loss from margin-rescaling\n",
        "        q = cvxopt.matrix(-np.array(losses, dtype=np.float))\n",
        "        # constraints: all alpha must be >zero\n",
        "        idy = np.identity(n_constraints)\n",
        "        tmp1 = np.zeros(n_constraints)\n",
        "        # positivity constraints:\n",
        "        if self.negativity_constraint is None:\n",
        "            # empty constraints\n",
        "            zero_constr = np.zeros(0)\n",
        "            joint_features_constr = np.zeros((0, n_constraints))\n",
        "        else:\n",
        "            joint_features_constr = joint_feature_matrix.T[self.negativity_constraint]\n",
        "            zero_constr = np.zeros(len(self.negativity_constraint))\n",
        "\n",
        "        # put together\n",
        "        G = cvxopt.sparse(cvxopt.matrix(np.vstack((-idy, joint_features_constr))))\n",
        "        h = cvxopt.matrix(np.hstack((tmp1, zero_constr)))\n",
        "\n",
        "        # equality constraint: sum of all alpha must be = C\n",
        "        A = cvxopt.matrix(np.ones((1, n_constraints)))\n",
        "        b = cvxopt.matrix([C])\n",
        "\n",
        "        # solve QP model\n",
        "        cvxopt.solvers.options['feastol'] = 1e-5\n",
        "        try:\n",
        "            solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "        except ValueError:\n",
        "            solution = {'status': 'error'}\n",
        "        if solution['status'] != \"optimal\":\n",
        "            print(\"regularizing QP!\")\n",
        "            P = cvxopt.matrix(np.dot(joint_feature_matrix, joint_feature_matrix.T)\n",
        "                              + 1e-8 * np.eye(joint_feature_matrix.shape[0]))\n",
        "            solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "            if solution['status'] != \"optimal\":\n",
        "                raise ValueError(\"QP solver failed. Try regularizing your QP.\")\n",
        "\n",
        "        # Lagrange multipliers\n",
        "        a = np.ravel(solution['x'])\n",
        "        self.old_solution = solution\n",
        "        self.prune_constraints(constraints, a)\n",
        "\n",
        "        # Support vectors have non zero lagrange multipliers\n",
        "        sv = a > self.inactive_threshold * C\n",
        "        if self.verbose > 1:\n",
        "            print(\"%d support vectors out of %d points\" % (np.sum(sv),\n",
        "                                                           n_constraints))\n",
        "        self.w = np.dot(a, joint_feature_matrix)\n",
        "        # we needed to flip the sign to make the dual into a minimization\n",
        "        # model\n",
        "        return -solution['primal objective']\n",
        "\n",
        "    def prune_constraints(self, constraints, a):\n",
        "        # append list for new constraint\n",
        "        self.alphas.append([])\n",
        "        assert(len(self.alphas) == len(constraints))\n",
        "        for constraint, alpha in zip(self.alphas, a):\n",
        "            constraint.append(alpha)\n",
        "            constraint = constraint[-self.inactive_window:]\n",
        "\n",
        "        # prune unused constraints:\n",
        "        # if the max of alpha in last 50 iterations was small, throw away\n",
        "        if self.inactive_window != 0:\n",
        "            max_active = [np.max(constr[-self.inactive_window:])\n",
        "                          for constr in self.alphas]\n",
        "            # find strongest constraint that is not ground truth constraint\n",
        "            strongest = np.max(max_active[1:])\n",
        "            inactive = np.where(max_active\n",
        "                                < self.inactive_threshold * strongest)[0]\n",
        "\n",
        "            for idx in reversed(inactive):\n",
        "                # if we don't reverse, we'll mess the indices up\n",
        "                del constraints[idx]\n",
        "                del self.alphas[idx]\n",
        "\n",
        "    def _check_bad_constraint(self, violation, djoint_feature_mean, loss,\n",
        "                              old_constraints, break_on_bad, tol=None):\n",
        "        violation_difference = violation - self.last_slack_\n",
        "        if self.verbose > 1:\n",
        "            print(\"New violation: %f difference to last: %f\"\n",
        "                  % (violation, violation_difference))\n",
        "        if violation_difference < 0 and violation > 0 and break_on_bad:\n",
        "            raise ValueError(\"Bad inference: new violation is smaller than\"\n",
        "                             \" old.\")\n",
        "        if tol is None:\n",
        "            tol = self.tol\n",
        "        if violation_difference < tol:\n",
        "            if self.verbose:\n",
        "                print(\"new constraint too weak.\")\n",
        "            return True\n",
        "        equals = [True for djoint_feature_, loss_ in old_constraints\n",
        "                  if (np.all(djoint_feature_ == djoint_feature_mean) and loss == loss_)]\n",
        "\n",
        "        if np.any(equals):\n",
        "            return True\n",
        "\n",
        "        if self.check_constraints:\n",
        "            for con in old_constraints:\n",
        "                # compute violation for old constraint\n",
        "                violation_tmp = max(con[1] - np.dot(self.w, con[0]), 0)\n",
        "                if self.verbose > 5:\n",
        "                    print(\"violation old constraint: %f\" % violation_tmp)\n",
        "                # if violation of new constraint is smaller or not\n",
        "                # significantly larger, don't add constraint.\n",
        "                # if smaller, complain about approximate inference.\n",
        "                if violation - violation_tmp < -1e-5:\n",
        "                    if self.verbose:\n",
        "                        print(\"bad inference: %f\" % (violation_tmp - violation))\n",
        "                    if break_on_bad:\n",
        "                        raise ValueError(\"Bad inference: new violation is\"\n",
        "                                         \" weaker than previous constraint.\")\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @classmethod\n",
        "    def constraint_equal(cls, y_1, y_2):\n",
        "        \"\"\"\n",
        "        This now more complex. y_1 and/or y_2 (I think) can be: array, pair of\n",
        "        arrays, pair of list of arrays (multitype)\n",
        "        We need to compare those!\n",
        "        \"\"\"\n",
        "        if isinstance(y_1, tuple):\n",
        "            # y_1 is relaxed Y\n",
        "            # y_1 and y_2 might be lists of ndarray (multitype) instead of\n",
        "            #    ndarray (single type)\n",
        "            u_m_1, pw_m_1 = y_1\n",
        "            if isinstance(y_2, tuple):  # we then compare two relaxed Ys\n",
        "                u_m_2, pw_m_2 = y_2\n",
        "                # now, do we multitype or single type relaxed marginals??\n",
        "                if isinstance(u_m_1, list):\n",
        "                    return all(np.all(_um1 == _um2) for _um1, _um2\n",
        "                               in zip( u_m_1,  u_m_2)) \\\n",
        "                        and all(np.all(_pw1 == _pw2) for _pw1, _pw2\n",
        "                                in zip(pw_m_1, pw_m_2))\n",
        "                else:\n",
        "                    return np.all(u_m_1 == u_m_2) and np.all(pw_m_1, pw_m_2)\n",
        "            else:\n",
        "                # NOTE original code was possibly comparing array and scalar\n",
        "                # return np.all(y_1[0] == y_2[0]) and np.all(y_1[1] == y_2[1])\n",
        "                return False\n",
        "        # might compare array and tuple... :-/  Was like that, I keep\n",
        "        return np.all(y_1 == y_2)\n",
        "\n",
        "    def _update_cache(self, X, Y, Y_hat):\n",
        "        \"\"\"Updated cached constraints.\"\"\"\n",
        "        if self.inference_cache == 0:\n",
        "            return\n",
        "        if (not hasattr(self, \"inference_cache_\")\n",
        "                or self.inference_cache_ is None):\n",
        "            self.inference_cache_ = [[] for y in Y_hat]\n",
        "\n",
        "        for sample, x, y, y_hat in zip(self.inference_cache_, X, Y, Y_hat):\n",
        "            already_there = [self.constraint_equal(y_hat, cache[2])\n",
        "                             for cache in sample]\n",
        "            if np.any(already_there):\n",
        "                continue\n",
        "            if len(sample) > self.inference_cache:\n",
        "                sample.pop(0)\n",
        "            # we computed both of these before, but summed them up immediately\n",
        "            # this makes it a little less efficient in the caching case.\n",
        "            # the idea is that if we cache, inference is way more expensive\n",
        "            # and this doesn't matter much.\n",
        "            sample.append((self.model.joint_feature(x, y_hat),\n",
        "                           self.model.loss(y, y_hat), y_hat))\n",
        "\n",
        "    def _constraint_from_cache(self, X, Y, joint_feature_gt, constraints):\n",
        "        if (not getattr(self, 'inference_cache_', False) or\n",
        "                self.inference_cache_ is False):\n",
        "            if self.verbose > 10:\n",
        "                print(\"Empty cache.\")\n",
        "            raise NoConstraint\n",
        "        gap = self.primal_objective_curve_[-1] - self.objective_curve_[-1]\n",
        "        if (self.cache_tol == 'auto' and gap < self.cache_tol_):\n",
        "            # do inference if gap has become to small\n",
        "            if self.verbose > 1:\n",
        "                print(\"Last gap too small (%f < %f), not loading constraint\"\n",
        "                      \" from cache.\"\n",
        "                      % (gap, self.cache_tol_))\n",
        "            raise NoConstraint\n",
        "\n",
        "        Y_hat = []\n",
        "        joint_feature_acc = np.zeros(self.model.size_joint_feature)\n",
        "        loss_mean = 0\n",
        "        for cached in self.inference_cache_:\n",
        "            # cached has entries of form (joint_feature, loss, y_hat)\n",
        "            violations = [np.dot(joint_feature, self.w) + loss\n",
        "                          for joint_feature, loss, _ in cached]\n",
        "            joint_feature, loss, y_hat = cached[np.argmax(violations)]\n",
        "            Y_hat.append(y_hat)\n",
        "            joint_feature_acc += joint_feature\n",
        "            loss_mean += loss\n",
        "\n",
        "        djoint_feature = (joint_feature_gt - joint_feature_acc) / len(X)\n",
        "        loss_mean = loss_mean / len(X)\n",
        "\n",
        "        violation = loss_mean - np.dot(self.w, djoint_feature)\n",
        "        if self._check_bad_constraint(violation, djoint_feature, loss_mean, constraints,\n",
        "                                      break_on_bad=False):\n",
        "            if self.verbose > 1:\n",
        "                print(\"No constraint from cache.\")\n",
        "            raise NoConstraint\n",
        "        return Y_hat, djoint_feature, loss_mean\n",
        "\n",
        "    def _find_new_constraint(self, X, Y, joint_feature_gt, constraints, check=True):\n",
        "        if self.n_jobs != 1:\n",
        "            # do inference in parallel\n",
        "            verbose = max(0, self.verbose - 3)\n",
        "            Y_hat = Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n",
        "                delayed(loss_augmented_inference)(\n",
        "                    self.model, x, y, self.w, relaxed=True)\n",
        "                for x, y in zip(X, Y))\n",
        "        else:\n",
        "            Y_hat = self.model.batch_loss_augmented_inference(\n",
        "                X, Y, self.w, relaxed=True)\n",
        "        # compute the mean over joint_features and losses\n",
        "\n",
        "        if getattr(self.model, 'rescale_C', False):\n",
        "            djoint_feature = (joint_feature_gt\n",
        "                              - self.model.batch_joint_feature(X, Y_hat, Y)) / len(X)\n",
        "        else:\n",
        "            djoint_feature = (joint_feature_gt\n",
        "                              - self.model.batch_joint_feature(X, Y_hat)) / len(X)\n",
        "\n",
        "        loss_mean = np.mean(self.model.batch_loss(Y, Y_hat))\n",
        "\n",
        "        violation = loss_mean - np.dot(self.w, djoint_feature)\n",
        "        if check and self._check_bad_constraint(\n",
        "                violation, djoint_feature, loss_mean, constraints,\n",
        "                break_on_bad=self.break_on_bad):\n",
        "            raise NoConstraint\n",
        "        return Y_hat, djoint_feature, loss_mean\n",
        "\n",
        "    def fit(self, X, Y, constraints=None, warm_start=False, initialize=True):\n",
        "        \"\"\"Learn parameters using cutting plane method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : iterable\n",
        "            Traing instances. Contains the structured input objects.\n",
        "            No requirement on the particular form of entries of X is made.\n",
        "\n",
        "        Y : iterable\n",
        "            Training labels. Contains the strctured labels for inputs in X.\n",
        "            Needs to have the same length as X.\n",
        "\n",
        "        contraints : ignored\n",
        "\n",
        "        warm_start : bool, default=False\n",
        "            Whether we are warmstarting from a previous fit.\n",
        "\n",
        "        initialize : boolean, default=True\n",
        "            Whether to initialize the model for the data.\n",
        "            Leave this true except if you really know what you are doing.\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(\"Training 1-slack dual structural SVM\")\n",
        "        cvxopt.solvers.options['show_progress'] = self.verbose > 3\n",
        "        if initialize:\n",
        "            self.model.initialize(X, Y)\n",
        "\n",
        "        # parse cache_tol parameter\n",
        "        if self.cache_tol is None or self.cache_tol == 'auto':\n",
        "            self.cache_tol_ = self.tol\n",
        "        else:\n",
        "            self.cache_tol_ = self.cache_tol\n",
        "\n",
        "        if not warm_start:\n",
        "            self.w = np.zeros(self.model.size_joint_feature)\n",
        "            constraints = []\n",
        "            self.objective_curve_, self.primal_objective_curve_ = [], []\n",
        "            self.cached_constraint_ = []\n",
        "            self.alphas = []  # dual solutions\n",
        "            # append constraint given by ground truth to make our life easier\n",
        "            constraints.append((np.zeros(self.model.size_joint_feature), 0))\n",
        "            self.alphas.append([self.C])\n",
        "            self.inference_cache_ = None\n",
        "            self.timestamps_ = [time()]\n",
        "        elif warm_start == \"soft\":\n",
        "            self.w = np.zeros(self.model.size_joint_feature)\n",
        "            constraints = []\n",
        "            self.alphas = []  # dual solutions\n",
        "            # append constraint given by ground truth to make our life easier\n",
        "            constraints.append((np.zeros(self.model.size_joint_feature), 0))\n",
        "            self.alphas.append([self.C])\n",
        "\n",
        "        else:\n",
        "            constraints = self.constraints_\n",
        "\n",
        "        self.last_slack_ = -1\n",
        "\n",
        "        # get the joint_feature of the ground truth\n",
        "        if getattr(self.model, 'rescale_C', False):\n",
        "            joint_feature_gt = self.model.batch_joint_feature(X, Y, Y)\n",
        "        else:\n",
        "            joint_feature_gt = self.model.batch_joint_feature(X, Y)\n",
        "\n",
        "        try:\n",
        "            # catch ctrl+c to stop training\n",
        "\n",
        "            for iteration in range(self.max_iter):\n",
        "                # main loop\n",
        "                cached_constraint = False\n",
        "                if self.verbose > 0:\n",
        "                    print(\"iteration %d\" % iteration)\n",
        "                if self.verbose > 2:\n",
        "                    print(self)\n",
        "                try:\n",
        "                    Y_hat, djoint_feature, loss_mean = self._constraint_from_cache(\n",
        "                        X, Y, joint_feature_gt, constraints)\n",
        "                    cached_constraint = True\n",
        "                except NoConstraint:\n",
        "                    try:\n",
        "                        Y_hat, djoint_feature, loss_mean = self._find_new_constraint(\n",
        "                            X, Y, joint_feature_gt, constraints)\n",
        "                        self._update_cache(X, Y, Y_hat)\n",
        "                    except NoConstraint:\n",
        "                        if self.verbose:\n",
        "                            print(\"no additional constraints\")\n",
        "                        if (self.switch_to is not None\n",
        "                                and self.model.inference_method !=\n",
        "                                self.switch_to):\n",
        "                            if self.verbose:\n",
        "                                print(\"Switching to %s inference\" %\n",
        "                                      str(self.switch_to))\n",
        "                            self.model.inference_method_ = \\\n",
        "                                self.model.inference_method\n",
        "                            self.model.inference_method = self.switch_to\n",
        "                            continue\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                self.timestamps_.append(time() - self.timestamps_[0])\n",
        "                self._compute_training_loss(X, Y, iteration)\n",
        "                constraints.append((djoint_feature, loss_mean))\n",
        "\n",
        "                # compute primal objective\n",
        "                last_slack = -np.dot(self.w, djoint_feature) + loss_mean\n",
        "                primal_objective = (self.C * len(X)\n",
        "                                    * max(last_slack, 0)\n",
        "                                    + np.sum(self.w ** 2) / 2)\n",
        "                self.primal_objective_curve_.append(primal_objective)\n",
        "                self.cached_constraint_.append(cached_constraint)\n",
        "\n",
        "                objective = self._solve_1_slack_qp(constraints,\n",
        "                                                   n_samples=len(X))\n",
        "\n",
        "                # update cache tolerance if cache_tol is auto:\n",
        "                if self.cache_tol == \"auto\" and not cached_constraint:\n",
        "                    self.cache_tol_ = (primal_objective - objective) / 4\n",
        "\n",
        "                self.last_slack_ = np.max([(-np.dot(self.w, djoint_feature) + loss_mean)\n",
        "                                           for djoint_feature, loss_mean in constraints])\n",
        "                self.last_slack_ = max(self.last_slack_, 0)\n",
        "\n",
        "                if self.verbose > 0:\n",
        "                    # the cutting plane objective can also be computed as\n",
        "                    # self.C * len(X) * self.last_slack_ + np.sum(self.w**2)/2\n",
        "                    print(\"cutting plane objective: %f, primal objective %f\"\n",
        "                          % (objective, primal_objective))\n",
        "                # we only do this here because we didn't add the gt to the\n",
        "                # constraints, which makes the dual behave a bit oddly\n",
        "                self.objective_curve_.append(objective)\n",
        "                self.constraints_ = constraints\n",
        "                if self.logger is not None:\n",
        "                    self.logger(self, iteration)\n",
        "\n",
        "                if self.verbose > 5:\n",
        "                    print(self.w)\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n",
        "        if self.verbose and self.n_jobs == 1:\n",
        "            print(\"calls to inference: %d\" % self.model.inference_calls)\n",
        "        # compute final objective:\n",
        "        self.timestamps_.append(time() - self.timestamps_[0])\n",
        "        primal_objective = self._objective(X, Y)\n",
        "        self.primal_objective_curve_.append(primal_objective)\n",
        "        self.objective_curve_.append(objective)\n",
        "        self.cached_constraint_.append(False)\n",
        "\n",
        "        if self.logger is not None:\n",
        "            self.logger(self, 'final')\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print(\"final primal objective: %f gap: %f\"\n",
        "                  % (primal_objective, primal_objective - objective))\n",
        "\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5inicW8e859"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "from joblib import Parallel, delayed, cpu_count\n",
        "from sklearn.utils import gen_even_slices, shuffle\n",
        "\n",
        "\n",
        "\n",
        "class SubgradientSSVM(BaseSSVM):\n",
        "    \"\"\"Structured SVM solver using subgradient descent.\n",
        "\n",
        "    Implements a margin rescaled with l1 slack penalty.\n",
        "    By default, a constant learning rate is used.\n",
        "    It is also possible to use the adaptive learning rate found by AdaGrad.\n",
        "\n",
        "    This class implements online subgradient descent. If n_jobs != 1,\n",
        "    small batches of size n_jobs are used to exploit parallel inference.\n",
        "    If inference is fast, use n_jobs=1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : StructuredModel\n",
        "        Object containing model structure. Has to implement\n",
        "        `loss`, `inference` and `loss_augmented_inference`.\n",
        "\n",
        "    max_iter : int, default=100\n",
        "        Maximum number of passes over dataset to find constraints and perform\n",
        "        updates.\n",
        "\n",
        "    C : float, default=1.\n",
        "        Regularization parameter.\n",
        "\n",
        "    verbose : int, default=0\n",
        "        Verbosity.\n",
        "\n",
        "    learning_rate : float or 'auto', default='auto'\n",
        "        Learning rate used in subgradient descent. If 'auto', the pegasos\n",
        "        schedule is used, which starts with ``learning_rate = n_samples * C``.\n",
        "\n",
        "    momentum : float, default=0.0\n",
        "        Momentum used in subgradient descent.\n",
        "\n",
        "    n_jobs : int, default=1\n",
        "        Number of parallel jobs for inference. -1 means as many as cpus.\n",
        "\n",
        "    batch_size : int, default=None\n",
        "        Ignored if n_jobs > 1. If n_jobs=1, inference will be done in mini\n",
        "        batches of size batch_size. If n_jobs=-1, batch learning will be\n",
        "        performed, that is the whole dataset will be used to compute each\n",
        "        subgradient.\n",
        "\n",
        "    show_loss_every : int, default=0\n",
        "        Controlls how often the hamming loss is computed (for monitoring\n",
        "        purposes). Zero means never, otherwise it will be computed very\n",
        "        show_loss_every'th epoch.\n",
        "\n",
        "    decay_exponent : float, default=1\n",
        "        Exponent for decaying learning rate. Effective learning rate is\n",
        "        ``learning_rate / (decay_t0 + t)** decay_exponent``. Zero means no\n",
        "        decay.\n",
        "\n",
        "    decay_t0 : float, default=10\n",
        "        Offset for decaying learning rate. Effective learning rate is\n",
        "        ``learning_rate / (decay_t0 + t)** decay_exponent``.\n",
        "\n",
        "    break_on_no_constraints : bool, default=True\n",
        "        Break when there are no new constraints found.\n",
        "\n",
        "    logger : logger object.\n",
        "\n",
        "    averaging : string, default=None\n",
        "        Whether and how to average weights. Possible options are 'linear',\n",
        "        'squared' and None.\n",
        "        The string reflects the weighting of the averaging:\n",
        "\n",
        "            - ``linear: w_avg ~ w_1 + 2 * w_2 + ... + t * w_t``\n",
        "\n",
        "            - ``squared: w_avg ~ w_1 + 4 * w_2 + ... + t**2 * w_t``\n",
        "\n",
        "        Uniform averaging is not implemented as it is worse than linear\n",
        "        weighted averaging or no averaging.\n",
        "\n",
        "    shuffle : bool, default=False\n",
        "        Whether to shuffle the dataset in each iteration.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    w : nd-array, shape=(model.size_joint_feature,)\n",
        "        The learned weights of the SVM.\n",
        "\n",
        "    ``loss_curve_`` : list of float\n",
        "        List of loss values if show_loss_every > 0.\n",
        "\n",
        "    ``objective_curve_`` : list of float\n",
        "       Primal objective after each pass through the dataset.\n",
        "\n",
        "    ``timestamps_`` : list of int\n",
        "       Total training time stored before each iteration.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    * Nathan Ratliff, J. Andrew Bagnell and Martin Zinkevich:\n",
        "        (Online) Subgradient Methods for Structured Prediction, AISTATS 2007\n",
        "\n",
        "    * Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan and Cotter,\n",
        "        Andrew: Pegasos: Primal estimated sub-gradient solver for svm,\n",
        "        Mathematical Programming 2011\n",
        "    \"\"\"\n",
        "    def __init__(self, model, max_iter=100, C=1.0, verbose=0, momentum=0.0,\n",
        "                 learning_rate='auto', n_jobs=1,\n",
        "                 show_loss_every=0, decay_exponent=1,\n",
        "                 break_on_no_constraints=True, logger=None, batch_size=None,\n",
        "                 decay_t0=10, averaging=None, shuffle=False):\n",
        "        BaseSSVM.__init__(self, model, max_iter, C, verbose=verbose,\n",
        "                          n_jobs=n_jobs, show_loss_every=show_loss_every,\n",
        "                          logger=logger)\n",
        "        self.averaging = averaging\n",
        "        self.break_on_no_constraints = break_on_no_constraints\n",
        "        self.momentum = momentum\n",
        "        self.learning_rate = learning_rate\n",
        "        self.t = 0\n",
        "        self.decay_exponent = decay_exponent\n",
        "        self.decay_t0 = decay_t0\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def _solve_subgradient(self, djoint_feature, n_samples, w):\n",
        "        \"\"\"Do a single subgradient step.\"\"\"\n",
        "        grad = (djoint_feature - w / (self.C * n_samples))\n",
        "\n",
        "        self.grad_old = ((1 - self.momentum) * grad\n",
        "                         + self.momentum * self.grad_old)\n",
        "        if self.decay_exponent == 0:\n",
        "            effective_lr = self.learning_rate_\n",
        "        else:\n",
        "            effective_lr = (self.learning_rate_\n",
        "                            / (self.t + self.decay_t0)\n",
        "                            ** self.decay_exponent)\n",
        "        w += effective_lr * self.grad_old\n",
        "\n",
        "        if self.averaging == 'linear':\n",
        "            rho = 2. / (self.t + 2.)\n",
        "            self.w = (1. - rho) * self.w + rho * w\n",
        "        elif self.averaging == 'squared':\n",
        "            rho = 6. * (self.t + 1) / ((self.t + 2) * (2 * self.t + 3))\n",
        "            self.w = (1. - rho) * self.w + rho * w\n",
        "        else:\n",
        "            self.w = w\n",
        "        self.t += 1.\n",
        "        return w\n",
        "\n",
        "    def fit(self, X, Y, constraints=None, warm_start=False, initialize=True):\n",
        "        \"\"\"Learn parameters using subgradient descent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : iterable\n",
        "            Traing instances. Contains the structured input objects.\n",
        "            No requirement on the particular form of entries of X is made.\n",
        "\n",
        "        Y : iterable\n",
        "            Training labels. Contains the strctured labels for inputs in X.\n",
        "            Needs to have the same length as X.\n",
        "\n",
        "        constraints : None\n",
        "            Discarded. Only for API compatibility currently.\n",
        "\n",
        "        warm_start : boolean, default=False\n",
        "            Whether to restart a previous fit.\n",
        "\n",
        "        initialize : boolean, default=True\n",
        "            Whether to initialize the model for the data.\n",
        "            Leave this true except if you really know what you are doing.\n",
        "        \"\"\"\n",
        "        if initialize:\n",
        "            self.model.initialize(X, Y)\n",
        "        if self.verbose:\n",
        "            print(\"Training primal subgradient structural SVM\")\n",
        "        self.grad_old = np.zeros(self.model.size_joint_feature)\n",
        "        self.w = getattr(self, \"w\", np.zeros(self.model.size_joint_feature))\n",
        "        w = self.w.copy()\n",
        "        if not warm_start:\n",
        "            self.objective_curve_ = []\n",
        "            self.timestamps_ = [time()]\n",
        "            if self.learning_rate == \"auto\":\n",
        "                self.learning_rate_ = self.C * len(X)\n",
        "            else:\n",
        "                self.learning_rate_ = self.learning_rate\n",
        "        else:\n",
        "            self.timestamps_ = (np.array(self.timestamps_) - time()).tolist()\n",
        "        try:\n",
        "            # catch ctrl+c to stop training\n",
        "            for iteration in range(self.max_iter):\n",
        "                if self.shuffle:\n",
        "                    X, Y = shuffle(X, Y)\n",
        "                if self.n_jobs == 1:\n",
        "                    objective, positive_slacks, w = self._sequential_learning(X, Y, w)\n",
        "                else:\n",
        "                    objective, positive_slacks, w = self._parallel_learning(X, Y, w)\n",
        "\n",
        "                # some statistics\n",
        "                objective = objective * self.C + np.sum(w ** 2) / 2.\n",
        "\n",
        "                if positive_slacks == 0:\n",
        "                    if self.verbose:\n",
        "                        print(\"No additional constraints\")\n",
        "                    if self.break_on_no_constraints:\n",
        "                        break\n",
        "                if self.verbose > 0:\n",
        "                    print(self)\n",
        "                    print(\"iteration %d\" % iteration)\n",
        "                    print(\"positive slacks: %d,\"\n",
        "                          \"objective: %f\" %\n",
        "                          (positive_slacks, objective))\n",
        "                self.timestamps_.append(time() - self.timestamps_[0])\n",
        "                self.objective_curve_.append(self._objective(X, Y))\n",
        "\n",
        "                if self.verbose > 2:\n",
        "                    print(self.w)\n",
        "\n",
        "                self._compute_training_loss(X, Y, iteration)\n",
        "                if self.logger is not None:\n",
        "                    self.logger(self, iteration)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Computing final objective\")\n",
        "\n",
        "        self.timestamps_.append(time() - self.timestamps_[0])\n",
        "        self.objective_curve_.append(self._objective(X, Y))\n",
        "        if self.logger is not None:\n",
        "            self.logger(self, 'final')\n",
        "        if self.verbose:\n",
        "            if self.objective_curve_:\n",
        "                print(\"final objective: %f\" % self.objective_curve_[-1])\n",
        "            if self.verbose and self.n_jobs == 1:\n",
        "                print(\"calls to inference: %d\" % self.model.inference_calls)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _parallel_learning(self, X, Y, w):\n",
        "        n_samples = len(X)\n",
        "        objective, positive_slacks = 0, 0\n",
        "        verbose = max(0, self.verbose - 3)\n",
        "        if self.batch_size is not None:\n",
        "            raise ValueError(\"If n_jobs != 1, batch_size needs to\"\n",
        "                             \"be None\")\n",
        "        # generate batches of size n_jobs\n",
        "        # to speed up inference\n",
        "        if self.n_jobs == -1:\n",
        "            n_jobs = cpu_count()\n",
        "        else:\n",
        "            n_jobs = self.n_jobs\n",
        "\n",
        "        n_batches = int(np.ceil(float(len(X)) / n_jobs))\n",
        "        slices = gen_even_slices(n_samples, n_batches)\n",
        "        for batch in slices:\n",
        "            X_b = X[batch]\n",
        "            Y_b = Y[batch]\n",
        "            candidate_constraints = Parallel(\n",
        "                n_jobs=self.n_jobs,\n",
        "                verbose=verbose)(delayed(find_constraint)(\n",
        "                    self.model, x, y, w)\n",
        "                    for x, y in zip(X_b, Y_b))\n",
        "            djoint_feature = np.zeros(self.model.size_joint_feature)\n",
        "            for x, y, constraint in zip(X_b, Y_b,\n",
        "                                        candidate_constraints):\n",
        "                y_hat, delta_joint_feature, slack, loss = constraint\n",
        "                if slack > 0:\n",
        "                    objective += slack\n",
        "                    djoint_feature += delta_joint_feature\n",
        "                    positive_slacks += 1\n",
        "            w = self._solve_subgradient(djoint_feature, n_samples, w)\n",
        "        return objective, positive_slacks, w\n",
        "\n",
        "    def _sequential_learning(self, X, Y, w):\n",
        "        n_samples = len(X)\n",
        "        objective, positive_slacks = 0, 0\n",
        "        if self.batch_size in [None, 1]:\n",
        "            # online learning\n",
        "            for x, y in zip(X, Y):\n",
        "                y_hat, delta_joint_feature, slack, loss = \\\n",
        "                    find_constraint(self.model, x, y, w)\n",
        "                objective += slack\n",
        "                if slack > 0:\n",
        "                    positive_slacks += 1\n",
        "                self._solve_subgradient(delta_joint_feature, n_samples, w)\n",
        "        else:\n",
        "            # mini batch learning\n",
        "            if self.batch_size == -1:\n",
        "                slices = [slice(0, len(X))]\n",
        "            else:\n",
        "                n_batches = int(np.ceil(float(len(X)) / self.batch_size))\n",
        "                slices = gen_even_slices(n_samples, n_batches)\n",
        "            for batch in slices:\n",
        "                X_b = X[batch]\n",
        "                Y_b = Y[batch]\n",
        "                Y_hat = self.model.batch_loss_augmented_inference(\n",
        "                    X_b, Y_b, w, relaxed=True)\n",
        "                delta_joint_feature = (self.model.batch_joint_feature(X_b, Y_b)\n",
        "                                       - self.model.batch_joint_feature(X_b, Y_hat))\n",
        "                loss = np.sum(self.model.batch_loss(Y_b, Y_hat))\n",
        "\n",
        "                violation = np.maximum(0, loss - np.dot(w, delta_joint_feature))\n",
        "                objective += violation\n",
        "                positive_slacks += self.batch_size\n",
        "                self._solve_subgradient(delta_joint_feature / len(X_b), n_samples, w)\n",
        "        return objective, positive_slacks, w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ousjXh4ee86B"
      },
      "outputs": [],
      "source": [
        "class CRF(StructuredModel):\n",
        "    \"\"\"Abstract base class\"\"\"\n",
        "    def __init__(self, n_states=None, n_features=None, inference_method=None,\n",
        "                 class_weight=None):\n",
        "        self.n_states = n_states\n",
        "        if inference_method is None:\n",
        "            # get first in list that is installed\n",
        "            inference_method = get_installed(['ad3', 'max-product', 'lp'])[0]\n",
        "        self.inference_method = inference_method\n",
        "        self.inference_calls = 0\n",
        "        self.n_features = n_features\n",
        "        self.class_weight = class_weight\n",
        "        self._set_size_joint_feature()\n",
        "        self._set_class_weight()\n",
        "\n",
        "    def initialize(self, X, Y):\n",
        "        # Works for both GridCRF and GraphCRF, but not ChainCRF.\n",
        "        # funny that ^^\n",
        "        n_features = X[0][0].shape[1]\n",
        "        if self.n_features is None:\n",
        "            self.n_features = n_features\n",
        "        elif self.n_features != n_features:\n",
        "            raise ValueError(\"Expected %d features, got %d\"\n",
        "                             % (self.n_features, n_features))\n",
        "\n",
        "        n_states = len(np.unique(np.hstack([y.ravel() for y in Y])))\n",
        "        if self.n_states is None:\n",
        "            self.n_states = n_states\n",
        "        elif self.n_states != n_states:\n",
        "            raise ValueError(\"Expected %d states, got %d\"\n",
        "                             % (self.n_states, n_states))\n",
        "\n",
        "        self._set_size_joint_feature()\n",
        "        self._set_class_weight()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\"%s(n_states: %s, inference_method: %s)\"\n",
        "                % (type(self).__name__, self.n_states,\n",
        "                   self.inference_method))\n",
        "\n",
        "    def _check_size_x(self, x):\n",
        "        features = self._get_features(x)\n",
        "        if features.shape[1] != self.n_features:\n",
        "            raise ValueError(\"Unary evidence should have %d feature per node,\"\n",
        "                             \" got %s instead.\"\n",
        "                             % (self.n_features, features.shape[1]))\n",
        "\n",
        "    def loss_augment_unaries(self, unary_potentials, y):\n",
        "        \"\"\"\n",
        "        we define it as a method so that subclasses can specialize it.\n",
        "        \"\"\"\n",
        "        loss_augment_unaries(unary_potentials, np.asarray(y),\n",
        "                             self.class_weight)\n",
        "\n",
        "    def loss_augmented_inference(self, x, y, w, relaxed=False,\n",
        "                                 return_energy=False):\n",
        "        \"\"\"Loss-augmented Inference for x relative to y using parameters w.\n",
        "\n",
        "        Finds (approximately)\n",
        "        armin_y_hat np.dot(w, joint_feature(x, y_hat)) + loss(y, y_hat)\n",
        "        using self.inference_method.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tuple\n",
        "            Instance of a graph with unary evidence.\n",
        "            x=(unaries, edges)\n",
        "            unaries are an nd-array of shape (n_nodes, n_features),\n",
        "            edges are an nd-array of shape (n_edges, 2)\n",
        "\n",
        "        y : ndarray, shape (n_nodes,)\n",
        "            Ground truth labeling relative to which the loss\n",
        "            will be measured.\n",
        "\n",
        "        w : ndarray, shape=(size_joint_feature,)\n",
        "            Parameters for the CRF energy function.\n",
        "\n",
        "        relaxed : bool, default=False\n",
        "            Whether relaxed inference should be performed.\n",
        "            Only meaningful if inference method is 'lp' or 'ad3'.\n",
        "            By default fractional solutions are rounded. If relaxed=True,\n",
        "            fractional solutions are returned directly.\n",
        "\n",
        "        return_energy : bool, default=False\n",
        "            Whether to return the energy of the solution (x, y) that was found.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray or tuple\n",
        "            By default an inter ndarray of shape=(n_nodes)\n",
        "            of variable assignments for x is returned.\n",
        "            If ``relaxed=True`` and inference_method is ``lp`` or ``ad3``,\n",
        "            a tuple (unary_marginals, pairwise_marginals)\n",
        "            containing the relaxed inference result is returned.\n",
        "            unary marginals is an array of shape (n_nodes, n_states),\n",
        "            pairwise_marginals is an array of\n",
        "            shape (n_states, n_states) of accumulated pairwise marginals.\n",
        "\n",
        "        \"\"\"\n",
        "        self.inference_calls += 1\n",
        "        self._check_size_w(w)\n",
        "        unary_potentials = self._get_unary_potentials(x, w)\n",
        "        pairwise_potentials = self._get_pairwise_potentials(x, w)\n",
        "        edges = self._get_edges(x)\n",
        "\n",
        "        self.loss_augment_unaries(unary_potentials, y)\n",
        "\n",
        "        return inference_dispatch(unary_potentials, pairwise_potentials, edges,\n",
        "                                  self.inference_method, relaxed=relaxed,\n",
        "                                  return_energy=return_energy)\n",
        "\n",
        "    def inference(self, x, w, relaxed=False, return_energy=False,\n",
        "                  constraints=None):\n",
        "        \"\"\"Inference for x using parameters w.\n",
        "\n",
        "        Finds (approximately)\n",
        "        armin_y np.dot(w, joint_feature(x, y))\n",
        "        using self.inference_method.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : tuple\n",
        "            Instance of a graph with unary evidence.\n",
        "            x=(unaries, edges)\n",
        "            unaries are an nd-array of shape (n_nodes, n_states),\n",
        "            edges are an nd-array of shape (n_edges, 2)\n",
        "\n",
        "        w : ndarray, shape=(size_joint_feature,)\n",
        "            Parameters for the CRF energy function.\n",
        "\n",
        "        relaxed : bool, default=False\n",
        "            Whether relaxed inference should be performed.\n",
        "            Only meaningful if inference method is 'lp' or 'ad3'.\n",
        "            By default fractional solutions are rounded. If relaxed=True,\n",
        "            fractional solutions are returned directly.\n",
        "\n",
        "        return_energy : bool, default=False\n",
        "            Whether to return the energy of the solution (x, y) that was found.\n",
        "\n",
        "        constraints : None or list, default=False\n",
        "            hard logic constraints, if any\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray or tuple\n",
        "            By default an inter ndarray of shape=(width, height)\n",
        "            of variable assignments for x is returned.\n",
        "            If ``relaxed=True`` and inference_method is ``lp`` or ``ad3``,\n",
        "            a tuple (unary_marginals, pairwise_marginals)\n",
        "            containing the relaxed inference result is returned.\n",
        "            unary marginals is an array of shape (width, height, n_states),\n",
        "            pairwise_marginals is an array of\n",
        "            shape (n_states, n_states) of accumulated pairwise marginals.\n",
        "\n",
        "        \"\"\"\n",
        "        self._check_size_w(w)\n",
        "        self.inference_calls += 1\n",
        "        unary_potentials = self._get_unary_potentials(x, w)\n",
        "        pairwise_potentials = self._get_pairwise_potentials(x, w)\n",
        "        edges = self._get_edges(x)\n",
        "\n",
        "        if constraints:\n",
        "            return inference_dispatch(unary_potentials, pairwise_potentials,\n",
        "                                      edges,\n",
        "                                      self.inference_method,\n",
        "                                      relaxed=relaxed,\n",
        "                                      return_energy=return_energy,\n",
        "                                      constraints=constraints)\n",
        "        else:\n",
        "            return inference_dispatch(unary_potentials, pairwise_potentials,\n",
        "                                      edges,\n",
        "                                      self.inference_method,\n",
        "                                      relaxed=relaxed,\n",
        "                                      return_energy=return_energy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzKF93nse86D"
      },
      "outputs": [],
      "source": [
        "class MultiLabelClf(CRF):\n",
        "    \"\"\"Multi-label model for predicting several binary classes.\n",
        "\n",
        "    Multi-label classification is a generalization of multi-class\n",
        "    classification, in that multiple classes can be present in each\n",
        "    example. This can also be thought of as predicting\n",
        "    binary indicator per class.\n",
        "\n",
        "    This class supports different models via the \"edges\" parameter.\n",
        "    Giving no edges yields independent classifiers for each class. Giving\n",
        "    \"full\" yields a fully connected graph over the labels, while \"tree\"\n",
        "    yields the best tree-shaped graph (using the Chow-Liu algorithm).\n",
        "    It is also possible to specify a custom connectivity structure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_labels : int (default=None)\n",
        "        Number of labels. Inferred from data if not provided.\n",
        "\n",
        "    n_features : int (default=None)\n",
        "        Number of input features. Inferred from data if not provided.\n",
        "\n",
        "    edges : array-like, string or None\n",
        "        Either None, which yields independent models, 'tree',\n",
        "        which yields the Chow-Liu tree over the labels, 'full',\n",
        "        which yields a fully connected graph, or an array-like\n",
        "        of edges for a custom dependency structure.\n",
        "\n",
        "    inference_method :\n",
        "        The inference method to be used.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, n_labels=None, n_features=None, edges=None,\n",
        "                 inference_method=None):\n",
        "        self.n_labels = n_labels\n",
        "        self.edges = edges\n",
        "        CRF.__init__(self, 2, n_features, inference_method)\n",
        "\n",
        "    def _set_size_joint_feature(self):\n",
        "        # try to set the size of joint_feature if possible\n",
        "        if self.n_features is not None and self.n_states is not None:\n",
        "            if self.edges is None:\n",
        "                self.edges = np.zeros(shape=(0, 2), dtype=np.int)\n",
        "            self.size_joint_feature = (self.n_features * self.n_labels + 4 *\n",
        "                                       self.edges.shape[0])\n",
        "\n",
        "    def initialize(self, X, Y):\n",
        "        n_features = X.shape[1]\n",
        "        if self.n_features is None:\n",
        "            self.n_features = n_features\n",
        "        elif self.n_features != n_features:\n",
        "            raise ValueError(\"Expected %d features, got %d\"\n",
        "                             % (self.n_features, n_features))\n",
        "\n",
        "        n_labels = Y.shape[1]\n",
        "        if self.n_labels is None:\n",
        "            self.n_labels = n_labels\n",
        "        elif self.n_labels != n_labels:\n",
        "            raise ValueError(\"Expected %d labels, got %d\"\n",
        "                             % (self.n_labels, n_labels))\n",
        "\n",
        "        self._set_size_joint_feature()\n",
        "        self._set_class_weight()\n",
        "\n",
        "    def _get_edges(self, x):\n",
        "        return self.edges\n",
        "\n",
        "    def _get_unary_potentials(self, x, w):\n",
        "        unary_params = w[:self.n_labels * self.n_features].reshape(\n",
        "            self.n_labels, self.n_features)\n",
        "        unary_potentials = np.dot(x, unary_params.T)\n",
        "        return np.vstack([-unary_potentials, unary_potentials]).T\n",
        "\n",
        "    def _get_pairwise_potentials(self, x, w):\n",
        "        pairwise_params = w[self.n_labels * self.n_features:].reshape(\n",
        "            self.edges.shape[0], self.n_states, self.n_states)\n",
        "        return pairwise_params\n",
        "\n",
        "    def joint_feature(self, x, y):\n",
        "        if isinstance(y, tuple):\n",
        "            # continuous pairwise marginals\n",
        "            y_cont, pairwise_marginals = y\n",
        "            y_signs = 2 * y_cont[:, 1] - 1\n",
        "            unary_marginals = np.repeat(x[np.newaxis, :], len(y_signs), axis=0)\n",
        "            unary_marginals *= y_signs[:, np.newaxis]\n",
        "        else:\n",
        "            # discrete y\n",
        "            y_signs = 2 * y - 1\n",
        "            unary_marginals = np.repeat(x[np.newaxis, :], len(y_signs), axis=0)\n",
        "            unary_marginals *= y_signs[:, np.newaxis]\n",
        "            pairwise_marginals = []\n",
        "            for edge in self.edges:\n",
        "                # indicator of one of four possible states of the edge\n",
        "                pw = np.zeros((2, 2))\n",
        "                pw[y[edge[0]], y[edge[1]]] = 1\n",
        "                pairwise_marginals.append(pw)\n",
        "\n",
        "        if len(pairwise_marginals):\n",
        "            pairwise_marginals = np.vstack(pairwise_marginals)\n",
        "            return np.hstack([unary_marginals.ravel(),\n",
        "                              pairwise_marginals.ravel()])\n",
        "        return unary_marginals.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQplSLvZe86E"
      },
      "outputs": [],
      "source": [
        "from os.path import dirname, join\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _safe_unpickle(file_name):\n",
        "    with open(file_name, \"rb\") as data_file:\n",
        "        if sys.version_info >= (3, 0):\n",
        "            # python3 unpickling of python2 unicode\n",
        "            data = pickle.load(data_file, encoding=\"latin1\")\n",
        "        else:\n",
        "            data = pickle.load(data_file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_letters():\n",
        "    \"\"\"Load the OCR letters dataset.\n",
        "\n",
        "    This is a chain classification task.\n",
        "    Each example consists of a word, segmented into letters.\n",
        "    The first letter of each word is ommited from the data,\n",
        "    as it was a capital letter (in contrast to all other letters).\n",
        "\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf\n",
        "    http://groups.csail.mit.edu/sls/archives/root/publications/1995/Kassel%20Thesis.pdf\n",
        "    http://www.seas.upenn.edu/~taskar/ocr/\n",
        "    \"\"\"\n",
        "    #module_path = dirname(__file__)\n",
        "    #module_path = r\"C:\\Users\\\\OneDrive - Technion\\Documents\\ \\ 2\\PROJ\\pystruct-master\\pystruct\\datasets\"\n",
        "    #module_path = r\"C:\\Users\\noamg\\Documents\\school\\technion\\doctorat courses\\ML2\\project\\pythonProject\"\n",
        "    module_path=\"/\"\n",
        "\n",
        "    data = _safe_unpickle(join(module_path, 'letters.pickle'))\n",
        "    # we add an easy to use image representation:\n",
        "    data['images'] = [np.hstack([l.reshape(16, 8) for l in word])\n",
        "                      for word in data['data']]\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_scene():\n",
        "    \"\"\"Load the scene multi-label dataset.\n",
        "\n",
        "    This is a benchmark multilabel dataset.\n",
        "    n_classes = 6\n",
        "    n_fetures = 294\n",
        "    n_samples_test = 1196\n",
        "    n_samples_train = 1211\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown.\n",
        "    Learning multi-label scene classification.\n",
        "    \"\"\"\n",
        "    #module_path = r\"C:\\Users\\\\OneDrive - Technion\\Documents\\ \\ 2\\PROJ\\pystruct-master\\pystruct\\datasets\"\n",
        "   # module_path = r\"C:\\Users\\noamg\\Documents\\school\\technion\\doctorat courses\\ML2\\project\\pythonProject\"\n",
        "    module_path=\"/\"\n",
        "    return _safe_unpickle(join(module_path, 'scene.pickle'))\n",
        "\n",
        "\n",
        "def load_snakes():\n",
        "    \"\"\"Load the synthetic snake datasets.\n",
        "\n",
        "    Taken from:\n",
        "    Nowozin, S., Rother, C., Bagon, S., Sharp, T., Yao, B., & Kohli, P.\n",
        "    Decision Tree Fields, ICCV 2011\n",
        "\n",
        "    This is a 2d grid labeling task where conditinal pairwise interactions are\n",
        "    important.\n",
        "    See the reference for an explanation.\n",
        "    \"\"\"\n",
        "   # module_path = r\"C:\\Users\\\\OneDrive - Technion\\Documents\\ \\ 2\\PROJ\\pystruct-master\\pystruct\\datasets\"\n",
        "    #module_path = r\"C:\\Users\\noamg\\Documents\\school\\technion\\doctorat courses\\ML2\\project\\pythonProject\"\n",
        "    #module_path = dirname(__file__)\n",
        "    module_path=\"/\"\n",
        "    return _safe_unpickle(join(module_path, 'snakes.pickle'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "scene = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PVIZl8ScKdSy",
        "outputId": "d734ed82-abd2-4d5e-9fff-1d8376c50631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36e7b9ee-db60-4124-9351-da00ee7dcef9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36e7b9ee-db60-4124-9351-da00ee7dcef9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving scene.pickle to scene.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GAizkcye86F"
      },
      "outputs": [],
      "source": [
        "scene=load_scene()\n",
        "x_train_r, x_test_r = scene['X_train'], scene['X_test']\n",
        "y_train_r, y_test_r = scene['y_train'], scene['y_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbxjz5A-e86G",
        "outputId": "61352e3f-f09e-4a91-fa05-c59f594bb9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:153: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:161: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old model score 0.866\n"
          ]
        }
      ],
      "source": [
        "#start_1 = time()\n",
        "model1 = OneSlackSSVM(MultiLabelClf(),max_iter = 1000)\n",
        "model1.fit(x_train_r, y_train_r)\n",
        "score_1 = model1.score(x_test_r,y_test_r)\n",
        "print(f\"Old model score {score_1:.3f}\")\n",
        "#print(f\"Time for 100 iterations is {time()-start_1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qua9krV8e86H",
        "outputId": "7896011b-efa6-49e3-bbfc-2c3edf0daf16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New model score 0.790\n"
          ]
        }
      ],
      "source": [
        "#start_2 = time()\n",
        "model2 = SubgradientSSVM(MultiLabelClf(),max_iter = 1000,batch_size = 10)\n",
        "model2.fit(x_train_r, y_train_r)\n",
        "score_2 = model2.score(x_test_r, y_test_r)\n",
        "print(f\"New model score {score_2:.3f}\")\n",
        "#print(f\"Time for 10 epochs is {time()-start_2}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ml2_project.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}